{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f1198dfc-4947-4943-86f6-e2297be25406",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Databricks notebook source\n",
    "from pyspark.sql.types import *\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as f\n",
    "import pandas as pd\n",
    "from pyspark.ml.feature import CountVectorizer, Tokenizer as Tokenizer_feature, StringIndexer, VectorAssembler, OneHotEncoder, Word2Vec, HashingTF, IndexToString\n",
    "from pyspark.ml.linalg import SparseVector, Vectors\n",
    "import numpy as np\n",
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "from pyspark.ml.classification import RandomForestClassifier, LogisticRegression\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.sql.window import Window\n",
    "from datetime import datetime\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark.sql.functions import col, concat_ws, udf, lit\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import broadcast\n",
    "from pyspark.ml.functions import vector_to_array\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number\n",
    "from huggingface_hub import InferenceClient\n",
    "from huggingface_hub import login\n",
    "import time\n",
    "from pyspark.ml.linalg import VectorUDT, DenseVector\n",
    "import numpy as np\n",
    "import sparknlp\n",
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "from pyspark.sql.types import ArrayType, FloatType\n",
    "from pyspark.sql.functions import udf\n",
    "from sparknlp.base import DocumentAssembler, Finisher\n",
    "from sparknlp.annotator import Tokenizer, StopWordsCleaner, WordEmbeddingsModel, SentenceEmbeddings, BertEmbeddings, Word2VecModel\n",
    "from pyspark.ml.classification import MultilayerPerceptronClassificationModel\n",
    "from user_process import preprocess_data\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ### Pre process good profile data\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "def strip_and_choose_first(str_lst):\n",
    "    return str_lst.strip(\"[]\").split(\", \")[0]\n",
    "\n",
    "\n",
    "def process_education(degree, field, title):\n",
    "    # Extract degree, field, and school title from each education entry\n",
    "    degree = strip_and_choose_first(degree)\n",
    "    field = strip_and_choose_first(field)\n",
    "    title = strip_and_choose_first(title)\n",
    "    edu_details = f\"{degree} in {field} from {title}\"\n",
    "    return edu_details\n",
    "\n",
    "\n",
    "def preprocess_profiles(df):\n",
    "    \"\"\"\n",
    "    df: profiles dataframe, such that name, id, city,  experience, position are in the correct format\n",
    "    returns pre processed dataframe.\n",
    "    \"\"\"\n",
    "    jobs = df.select('name', 'id', 'city', f.col('experience')[0].getField('title').alias('job_title'), 'position')\n",
    "    process_education_udf = udf(process_education, StringType())\n",
    "    job_titles_df = jobs.select(\n",
    "        f.when(f.col('job_title').isNotNull(), f.lower(f.col('job_title')))\n",
    "        .otherwise(f.when(f.col('position').isNotNull(), f.lower(f.col('position'))).otherwise(f.lit('')))\n",
    "        .alias('processed_title'), 'id'\n",
    "    )\n",
    "\n",
    "    df = df.join(job_titles_df, on='id')\n",
    "    edu_filtered_df = df.filter((col(\"education\").isNotNull()) & (col(\"education\") != f.lit([])))\n",
    "    no_edu_df = df.filter((col(\"education\").isNull()) | (col(\"education\") == f.lit([])))\n",
    "\n",
    "    filtered_df = edu_filtered_df.withColumn('degree', col('education').getField('degree').cast('string'))\n",
    "    filtered_df = filtered_df.withColumn('field', col('education').getField('field').cast('string'))\n",
    "    filtered_df = filtered_df.withColumn('school', col('education').getField('title').cast('string'))\n",
    "\n",
    "    # Process the DataFrame\n",
    "    edu_filtered_df = filtered_df.withColumn(\"processed_education\", \n",
    "                                            process_education_udf(col('degree'), col('field'), col('school')))\n",
    "    no_edu_df = no_edu_df.withColumn(\"processed_education\", lit(''))\n",
    "    edu_filtered_df = edu_filtered_df.select(['id', 'processed_education', 'processed_title', 'name','city'])\n",
    "    no_edu_df = no_edu_df.select(['id', 'processed_education', 'processed_title', 'name','city'])\n",
    "    df = edu_filtered_df.union(no_edu_df)\n",
    "    return df \n",
    "\n",
    "def generate_small_good_sample(spark):\n",
    "    profiles_with_scores = spark.read.parquet(\"/Workspace/Users/lihi.kaspi@campus.technion.ac.il/user_profiles_with_scores.parquet\")\n",
    "    profiles = spark.read.parquet('/dbfs/linkedin_people_train_data')\n",
    "    profiles_with_scores = profiles_with_scores.withColumn(\n",
    "        'label', \n",
    "        f.when(f.col('profile_score') < 5, 0\n",
    "        ).when(f.col('profile_score') < 10, 1\n",
    "        ).when(f.col('profile_score') < 15, 2\n",
    "        ).when(f.col('profile_score') < 20, 3\n",
    "        ).otherwise(4)\n",
    "    )\n",
    "    df = preprocess_profiles(profiles_with_scores)\n",
    "    df = df.join(profiles_with_scores, on='id')\n",
    "    good_profiles_df = df.filter(col('label').isin([3,4])).select(['id','processed_education','processed_title', 'about'])\n",
    "    good_profiles_df = good_profiles_df.limit(10000)\n",
    "    good_profiles_df.write.mode(\"overwrite\").parquet(\"/Workspace/Users/lihi.kaspi@campus.technion.ac.il/sample_good_profile_data.parquet\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "def df_to_vector(df, good_profiles_df):\n",
    "    tokenizer_title = Tokenizer_feature(inputCol=\"processed_title\", outputCol=\"tokened_title\")\n",
    "    w2v_title = Word2Vec(inputCol=\"tokened_title\", outputCol=\"vector_title\", vectorSize=200, minCount=1)\n",
    "\n",
    "    tokenizer_edu = Tokenizer_feature(inputCol=\"processed_education\", outputCol=\"tokened_edu\")\n",
    "    w2v_edu = Word2Vec(inputCol=\"tokened_edu\", outputCol=\"vector_edu\", vectorSize=200, minCount=1)\n",
    "\n",
    "    pipeline = Pipeline(stages=[tokenizer_title, w2v_title, tokenizer_edu, w2v_edu])\n",
    "\n",
    "    model_vectorize = pipeline.fit(df)\n",
    "\n",
    "    # Create embeddings for job titles and centroids\n",
    "    df_with_vectors = model_vectorize.transform(df)\n",
    "    good_with_vectors = model_vectorize.transform(good_profiles_df)\n",
    "    return df_with_vectors, good_with_vectors\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "def cross_dfs(df_with_vectors, good_with_vectors):\n",
    "    profiles = df_with_vectors.withColumnRenamed(\"vector_title\", \"pos_embed\") \\\n",
    "                            .withColumnRenamed(\"vector_edu\", \"edu_embed\") \\\n",
    "                            .withColumnRenamed(\"id\", \"profiles_id\")\n",
    "\n",
    "    good_profiles = good_with_vectors.withColumnRenamed(\"vector_title\", \"pos_embed_good\") \\\n",
    "                                    .withColumnRenamed(\"vector_edu\", \"edu_embed_good\") \\\n",
    "                                    .withColumnRenamed(\"id\", \"good_profile_id\")\n",
    "\n",
    "    good_profiles = good_profiles.select([\"good_profile_id\", \"pos_embed_good\", \"edu_embed_good\"])\n",
    "    profiles = profiles.select([\"profiles_id\", \"pos_embed\", \"edu_embed\"])\n",
    "\n",
    "    profiles = profiles.withColumn(\"edu_embed\", vector_to_array(col(\"edu_embed\")))\n",
    "    profiles = profiles.withColumn(\"pos_embed\", vector_to_array(col(\"pos_embed\")))\n",
    "\n",
    "    good_profiles = good_profiles.withColumn(\"edu_embed_good\", vector_to_array(col(\"edu_embed_good\")))\n",
    "    good_profiles = good_profiles.withColumn(\"pos_embed_good\", vector_to_array(col(\"pos_embed_good\")))\n",
    "\n",
    "    good_profiles_broadcast = broadcast(good_profiles)\n",
    "\n",
    "    profiles_cross = profiles.join(good_profiles_broadcast, how=\"inner\")\n",
    "    return profiles_cross\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "\n",
    "def dot_product(vec1, vec2):\n",
    "    return F.expr(f\"\"\"\n",
    "        aggregate(transform({vec1}, (x, i) -> x * {vec2}[i]), 0D, (acc, x) -> acc + x)\n",
    "    \"\"\")\n",
    "\n",
    "def vector_norm(vec):\n",
    "    return F.sqrt(F.expr(f\"aggregate(transform({vec}, x -> x * x), 0D, (acc, x) -> acc + x)\"))\n",
    "def compute_sim(profiles_cross):\n",
    "    edu_dot_product = dot_product(\"edu_embed\", \"edu_embed_good\")\n",
    "    pos_dot_product = dot_product(\"pos_embed\", \"pos_embed_good\")\n",
    "\n",
    "\n",
    "    edu_norm_profile = vector_norm(\"edu_embed\")\n",
    "    edu_norm_good = vector_norm(\"edu_embed_good\")\n",
    "\n",
    "    pos_norm_profile = vector_norm(\"pos_embed\")\n",
    "    pos_norm_good = vector_norm(\"pos_embed_good\")\n",
    "\n",
    "    profiles_cross = profiles_cross.withColumn(\n",
    "        \"edu_sim\", edu_dot_product / (edu_norm_profile * edu_norm_good)\n",
    "    ).withColumn(\n",
    "        \"pos_sim\", pos_dot_product / (pos_norm_profile * pos_norm_good)\n",
    "    ).withColumn(\n",
    "        \"total_sim\", F.col(\"edu_sim\") + F.col(\"pos_sim\")\n",
    "    )\n",
    "    return profiles_cross\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "\n",
    "def get_best_matches(profiles_cross):\n",
    "    # order by highest similarity\n",
    "    window_spec = Window.partitionBy(\"profiles_id\").orderBy(col(\"total_sim\").desc())\n",
    "    # Rank the matches and filter to keep only the best match per profile\n",
    "    best_matches = profiles_cross.withColumn(\"rank\", row_number().over(window_spec)).filter(col(\"rank\") == 1)\n",
    "\n",
    "    best_matches = best_matches.select(\n",
    "        col(\"profiles_id\"),\n",
    "        col(\"good_profile_id\").alias(\"matched_good_profile_id\"),\n",
    "        col(\"total_sim\")\n",
    "    )\n",
    "    return best_matches\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "def get_match_df(best_matches, spark):\n",
    "    good_profiles = spark.read.parquet(\"/Workspace/Users/lihi.kaspi@campus.technion.ac.il/sample_good_profile_data.parquet\")\n",
    "    \n",
    "    good_profiles = good_profiles.select(['id','about']).dropna().withColumnRenamed('id', \"matched_good_profile_id\")\n",
    "\n",
    "    match_df = best_matches.join(good_profiles, on=\"matched_good_profile_id\")\n",
    "    return match_df\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "def generate_sections(bad_profile_df, match_df, spark):\n",
    "    access_token = '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'\n",
    "    login(access_token)\n",
    "\n",
    "\n",
    "    df = match_df.withColumnRenamed('profiles_id', 'id').join(bad_profile_df.withColumnRenamed('some_column_name', 'id'), on=\"id\")\n",
    "    pd_df = df.toPandas()\n",
    "    \n",
    "    def create_section(user_data, procesed_edu, city, name, proccesed_title):\n",
    "        client = InferenceClient(token=access_token)\n",
    "        input_prompt = f\"This is an about section of a user similar to me:{user_data}. build an about section for me. my name is {name}, I live in {city}. my education details are {procesed_edu} and my job title is {proccesed_title}.  Do not use things like [Assuming a similar role as Fleet Account Manager based on Josh's profession] Business Development Specialist at [Assuming a company similar to Knapheide Manufacturing], it should look like a real about section\"\n",
    "        completion = client.text_generation(\n",
    "            model=\"mistralai/Mistral-7B-Instruct-v0.3\", \n",
    "            prompt=input_prompt, \n",
    "            max_new_tokens=500\n",
    "        )\n",
    "        return completion\n",
    "    i = 0\n",
    "    abouts = []\n",
    "    for _, row in pd_df.iterrows():\n",
    "        print(i)\n",
    "        i+=1\n",
    "        user_data = row['about']\n",
    "        name  = row['name']\n",
    "        city = row['city']\n",
    "        proccesed_edu = row['processed_education']\n",
    "        proccesed_title = row['processed_title']\n",
    "        completion = create_section(user_data, proccesed_edu, city, name, proccesed_title)\n",
    "        time.sleep(2)\n",
    "        abouts.append((row[\"id\"],completion))\n",
    "        if i == 400:\n",
    "            break\n",
    "    generated_abouts_df = spark.createDataFrame(abouts, [\"id\", \"about\"])\n",
    "\n",
    "    return generated_abouts_df\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "def optimize_profiles(df,spark, good_profiles=None):\n",
    "    \"\"\"\n",
    "    Optimize the profiles based on the good profiles provided.\n",
    "    If no good_profiles are provided, default to using the sample good profiles we defined.\n",
    "    \"\"\"\n",
    "    if good_profiles is None:\n",
    "        good_profiles = spark.read.parquet(\"/Workspace/Users/lihi.kaspi@campus.technion.ac.il/sample_good_profile_data.parquet\")\n",
    "    df = preprocess_profiles(df)\n",
    "    df_vector, good_profiles_vector = df_to_vector(df, good_profiles)\n",
    "    profiles_cross = cross_dfs(df_vector, good_profiles_vector)\n",
    "    sim_cross = compute_sim(profiles_cross)\n",
    "    best_matches = get_best_matches(sim_cross)\n",
    "    match_df = get_match_df(best_matches, spark)\n",
    "    gen_df = generate_sections(df, match_df, spark)\n",
    "    return gen_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1d2aeae4-e686-4761-bea4-2cea9370082a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "generate_small_good_sample(spark)\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "# Call the function with the spark session\n",
    "generate_small_good_sample(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2606b06b-4a13-4902-99b1-5763a8f75565",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "small_bert_L2_128 download started this may take some time.\nApproximate size to download 16.1 MB\n\r[ | ]\r[OK!]\nInitial score: 2.0\n0\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>id</th><th>about</th></tr></thead><tbody><tr><td>1</td><td>.\n",
       "\n",
       "Lior Zaphir, Senior Data Scientist at Google, resides in New York, New York, US. With a Master of Science in Data Science from Harvard University, Lior brings a wealth of knowledge and expertise to his role. He specializes in leveraging advanced data analytics techniques to drive business growth and innovation.\n",
       "\n",
       "Lior's skillset includes proficiency in Python, R, SQL, and Java, as well as machine learning algorithms, data visualization, and predictive modeling. He has a strong background in statistical analysis, data mining, and big data processing.\n",
       "\n",
       "Prior to joining Google, Lior worked as a Data Scientist at IBM, where he developed predictive models for customer behavior and optimized marketing strategies. His work has been published in several academic journals and he has presented at various data science conferences.\n",
       "\n",
       "In his free time, Lior enjoys exploring the city, trying new restaurants, and playing chess. He is an active member of the data science community, regularly contributing to open-source projects and mentoring aspiring data scientists.</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1,
         ".\n\nLior Zaphir, Senior Data Scientist at Google, resides in New York, New York, US. With a Master of Science in Data Science from Harvard University, Lior brings a wealth of knowledge and expertise to his role. He specializes in leveraging advanced data analytics techniques to drive business growth and innovation.\n\nLior's skillset includes proficiency in Python, R, SQL, and Java, as well as machine learning algorithms, data visualization, and predictive modeling. He has a strong background in statistical analysis, data mining, and big data processing.\n\nPrior to joining Google, Lior worked as a Data Scientist at IBM, where he developed predictive models for customer behavior and optimized marketing strategies. His work has been published in several academic journals and he has presented at various data science conferences.\n\nIn his free time, Lior enjoys exploring the city, trying new restaurants, and playing chess. He is an active member of the data science community, regularly contributing to open-source projects and mentoring aspiring data scientists."
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "about",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "small_bert_L2_128 download started this may take some time.\nApproximate size to download 16.1 MB\n\r[ | ]\r[OK!]\nFinal score: 3.0\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import StructType, StructField, StringType, ArrayType, IntegerType, MapType\n",
    "\n",
    "user_input = False\n",
    "name = \"Lior Zaphir\" # String\n",
    "city = \"New York, New York, US\" # String in format \"City, State, Country\" \n",
    "title = \"Senior Data Scientist at Google\" # String\n",
    "degree = \"MSc\" # String  \n",
    "field = \"Data Science\" # String\n",
    "uni = 'Harvard University' # String \n",
    "about = None # String\n",
    "followers = 500 # Integer\n",
    "country_code = 'US' # String\n",
    "position = 'Senior Data Scientist at Google' # String\n",
    "languages = ['English', 'Spanish', 'French'] # List of strings\n",
    "recommendations_count = 1 # Integer\n",
    "df = ''\n",
    "if user_input:\n",
    "    print(\"Hello! Welcome to Profile Pro :)\")\n",
    "    print(\"How many profiles are we optimizing today?\")\n",
    "    n = input()\n",
    "    profile_dict = {}\n",
    "    for i in range(int(n)):\n",
    "        print(\"Please enter your details:\")\n",
    "        print(\"Name:\")\n",
    "        name = input()\n",
    "        print(\"City:(format is: Topeka, Kansas, United States)\")\n",
    "        city = input()\n",
    "\n",
    "        print(\"Degree:\")\n",
    "        degree= input()\n",
    "        print(\"Field:\")\n",
    "        field = input()\n",
    "        print(\"From:(for example: University of South Florida)\")\n",
    "        uni = input()\n",
    "        education = {\"degree\": degree, \"field\": field, \"title\": uni}\n",
    "        print(\"Title:(for example:Sales Manager )\")\n",
    "        title = input()\n",
    "        experience = {\"title\": title}\n",
    "        #print(\"Position:(Head of sales at X company)\")\n",
    "        #position = input()\n",
    "        print(\"If you currently have an about section, please enter it here. If not enter 'None'.\")\n",
    "        about = input()\n",
    "        if about == 'None':\n",
    "            about = None\n",
    "        print(\"To finish off: how many followers do you currently have?\")\n",
    "        followers = input()\n",
    "        profile_dict[i] = {\"name\": name, \"city\": city, \"education\": education, \"experience\": [experience], \"about\": about, \"followers\": followers}\n",
    "    data = [Row(id=k, **v) for k, v in profile_dict.items()]\n",
    "    df = spark.createDataFrame(data)\n",
    "else:\n",
    "    education = [{\"degree\": degree, \"field\": field, \"title\": uni}]\n",
    "    experience = [{\"title\": title}]\n",
    "    schema = StructType([\n",
    "     StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"city\", StringType(), True),\n",
    "    StructField(\"position\", StringType(), True),\n",
    "    StructField(\"country_code\", StringType(), True),\n",
    "    StructField(\"education\", ArrayType(StructType([\n",
    "        StructField(\"degree\", StringType(), True),\n",
    "        StructField(\"field\", StringType(), True),\n",
    "        StructField(\"title\", StringType(), True)\n",
    "])), True)\n",
    ",  # Nested dictionary\n",
    "    StructField(\"experience\", ArrayType(MapType(StringType(), StringType())), True),  # List of dicts\n",
    "    StructField(\"about\", StringType(), True),\n",
    "    StructField(\"followers\", IntegerType(), True),\n",
    "    StructField(\"languages\", ArrayType(StringType()), True),\n",
    "    StructField(\"recommendations_count\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "    # Create DataFrame\n",
    "    df = spark.createDataFrame([(1, name, city,position ,country_code, education, experience, about, followers, languages, recommendations_count)], schema)\n",
    "\n",
    "# Add profile_score column\n",
    "df = df.withColumn(\"profile_score\", lit(0))  # Assuming a default value of 0 for profile_score\n",
    "\n",
    "# Classify the profiles before generation\n",
    "features_df = preprocess_data(df)\n",
    "mlp_model = MultilayerPerceptronClassificationModel.load(\"/Workspace/Users/lihi.kaspi@campus.technion.ac.il/mlp_model\")\n",
    "predictions = mlp_model.transform(features_df)\n",
    "print(f\"Initial score: {predictions.select('prediction').first()[0]}\")\n",
    "\n",
    "optimized_df = optimize_profiles(df, spark)\n",
    "display(optimized_df)\n",
    "\n",
    "# Classify the profiles after generation\n",
    "after_df = optimized_df.join(df.drop('about'), on='id')\n",
    "features_df = preprocess_data(after_df)\n",
    "predictions = mlp_model.transform(features_df)\n",
    "print(f\"Final score: {predictions.select('prediction').first()[0]}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "User Input Optimization",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}