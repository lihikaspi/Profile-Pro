{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1fab96f5-4e49-4e0f-8ab4-f99138426a27",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as f\n",
    "import pandas as pd\n",
    "from pyspark.ml.feature import CountVectorizer, Tokenizer, StringIndexer, VectorAssembler, Tokenizer, OneHotEncoder, Word2Vec, HashingTF, IndexToString\n",
    "from pyspark.ml.linalg import SparseVector, Vectors\n",
    "import numpy as np\n",
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "from pyspark.ml.classification import RandomForestClassifier, LogisticRegression\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.sql.window import Window\n",
    "from datetime import datetime\n",
    "import re\n",
    "import shutil\n",
    "import os\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.functions import vector_to_array\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark.ml.classification import MultilayerPerceptronClassificationModel\n",
    "\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "193c15f3-486c-4da0-a974-1adad00b8cab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "profiles = spark.read.parquet('/dbfs/linkedin_people_train_data')\n",
    "\n",
    "# new df with processed vector to go into the model\n",
    "processed_data = spark.read.parquet(\"/Workspace/Users/lihi.kaspi@campus.technion.ac.il/processed_data.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7fae1d65-835e-4e64-b375-4a84ca79efc0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "processed_data = processed_data.withColumn(\n",
    "    'label', \n",
    "    f.when(f.col('profile_score') < 5, 0\n",
    "    ).when(f.col('profile_score') < 10, 1\n",
    "    ).when(f.col('profile_score') < 15, 2\n",
    "    ).when(f.col('profile_score') < 20, 3\n",
    "    ).otherwise(4)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "20a194b3-4589-4248-9e74-e6a02b0cc95a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "evaluator_accuracy = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\"\n",
    ")\n",
    "\n",
    "evaluator_f1 = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"f1\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "70ad52a4-0b96-4fbf-b3a8-a97fe1c75484",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train_df, test_df = processed_data.randomSplit([0.7, 0.3], seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a8accc34-7e9d-45f1-8ce4-298079dbd770",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Validate the training data\n",
    "train_df = train_df.na.drop()\n",
    "train_df = train_df.filter(f.size(vector_to_array(f.col('features'))) == 133)\n",
    "\n",
    "# Validate the test data\n",
    "test_df = test_df.na.drop()\n",
    "test_df = test_df.filter(f.size(vector_to_array(f.col('features'))) == 133)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e48687b1-9bda-4f83-9eaf-ece7d37ae670",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "model_path = 'dbfs:/Workspace/Users/lihi.kaspi@campus.technion.ac.il/mlp_model'\n",
    "mlp_model = MultilayerPerceptronClassificationModel.load(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a8750cee-de23-47c7-bf77-f30b6e89b04f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mlp_predictions = mlp_model.transform(test_df)\n",
    "\n",
    "accuracy = evaluator_accuracy.evaluate(mlp_predictions)\n",
    "f1_score = evaluator_f1.evaluate(mlp_predictions)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"F1-Score: {f1_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "67e83ba5-d706-465a-8501-95a82e4bcb79",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Convert predictions and actual labels to pandas DataFrame\n",
    "sample = mlp_predictions.select('prediction', 'label').toPandas()\n",
    "\n",
    "# Define the mapping of numeric values to category names\n",
    "category_mapping = {0: 'bad', 1: 'below average', 2: 'average', 3: 'above average', 4: 'good'}\n",
    "\n",
    "# Map numeric predictions and labels to category names\n",
    "sample['predicted_category'] = sample['prediction'].map(category_mapping)\n",
    "sample['actual_category'] = sample['label'].map(category_mapping)\n",
    "\n",
    "# Count occurrences of each category for predictions and actual labels\n",
    "pred_counts = sample['predicted_category'].value_counts().reindex(category_mapping.values(), fill_value=0)\n",
    "actual_counts = sample['actual_category'].value_counts().reindex(category_mapping.values(), fill_value=0)\n",
    "\n",
    "# Plot comparative bar chart\n",
    "categories = list(category_mapping.values())\n",
    "x = np.arange(len(categories))  # X positions for bars\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "bar_width = 0.4  # Width of bars\n",
    "\n",
    "plt.bar(x - bar_width/2, actual_counts.values, width=bar_width, label='real', color='#a2d5f2', edgecolor='black')\n",
    "plt.bar(x + bar_width/2, pred_counts.values, width=bar_width, label='Predicted', color='#f2aac7', edgecolor='black')\n",
    "\n",
    "# Formatting\n",
    "plt.xticks(x, categories, rotation=15)\n",
    "plt.xlabel('Profile Score')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Comparison of real vs. Predicted Profile Scores')\n",
    "plt.legend()\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b18f861a-06c7-4da9-aa56-e4de1a869d32",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Convert predictions and labels to pandas DataFrame\n",
    "sample = mlp_predictions.select('prediction', 'label').toPandas()\n",
    "\n",
    "# Define category mapping\n",
    "category_mapping = {0: 'bad', 1: 'below average', 2: 'average', 3: 'above average', 4: 'good'}\n",
    "\n",
    "# Compute classification report\n",
    "category_report = classification_report(sample['label'], sample['prediction'], output_dict=True)\n",
    "\n",
    "# Convert to DataFrame and remove overall metrics (last 3 rows)\n",
    "category_df = pd.DataFrame(category_report).T.iloc[:-3]\n",
    "\n",
    "# Map index (numeric classes) to category names\n",
    "category_df.index = [category_mapping[int(idx)] for idx in category_df.index]\n",
    "\n",
    "# Compute per-category accuracy (TP / Total Samples in Class)\n",
    "category_df[\"accuracy\"] = np.diag(pd.crosstab(sample[\"label\"], sample[\"prediction\"], normalize='index'))\n",
    "\n",
    "# Display results\n",
    "print(category_df[['precision', 'recall', 'f1-score', 'accuracy', 'support']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f745be78-b777-4e99-b6a2-10a6d5c21948",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(profiles.select(['name', 'about', 'current_company', 'experience', 'education', 'followers', 'position']).limit(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ea0a1a0-2ad2-43a3-b107-0328c83695c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, explode, lit, sequence, to_date, date_format, array_except, collect_list\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "\n",
    "# Step 1: Explode the experience array\n",
    "df_exploded = profiles.withColumn(\"experience_entry\", explode(col(\"experience\")))\n",
    "\n",
    "# Step 2: Extract and parse start_date and end_date\n",
    "df_parsed = df_exploded.select(\n",
    "    col(\"experience_entry.start_date\").alias(\"start_date\"),\n",
    "    col(\"experience_entry.end_date\").alias(\"end_date\"),\n",
    "    col(\"experience\").alias(\"original_experience\")\n",
    ").withColumn(\n",
    "    \"start_date\", to_date(col(\"start_date\"), \"MMM yyyy\")\n",
    ").withColumn(\n",
    "    \"end_date\", to_date(\n",
    "        when(col(\"end_date\") != \"Present\", col(\"end_date\"))\n",
    "        .otherwise(lit(\"2023-09-01\")),  # Assuming \"Present\" is the current date\n",
    "        \"MMM yyyy\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# Step 3: Compute the global earliest and latest dates per row\n",
    "global_dates = df_parsed.groupBy(\"original_experience\").agg(\n",
    "    f.min(\"start_date\").alias(\"global_start\"),\n",
    "    f.max(\"end_date\").alias(\"global_end\")\n",
    ")\n",
    "\n",
    "# Step 4: Generate a full timeline per row\n",
    "global_dates = global_dates.withColumn(\n",
    "    \"full_timeline\", sequence(col(\"global_start\"), col(\"global_end\"), f.expr(\"INTERVAL 1 MONTH\"))\n",
    ")\n",
    "\n",
    "# Explode the full_timeline and format the dates\n",
    "global_dates = global_dates.withColumn(\"full_month\", explode(col(\"full_timeline\"))).withColumn(\n",
    "    \"full_months\", date_format(col(\"full_month\"), \"yyyy-MM\")\n",
    ")\n",
    "\n",
    "# Step 5: Generate covered months for each experience\n",
    "covered_months = df_parsed.withColumn(\n",
    "    \"covered_range\", sequence(col(\"start_date\"), col(\"end_date\"), lit(\"1 month\").cast(\"interval\"))\n",
    ").withColumn(\n",
    "    \"covered_month\", explode(col(\"covered_range\"))\n",
    ").withColumn(\n",
    "    \"covered_months\", date_format(col(\"covered_month\"), \"yyyy-MM\")\n",
    ").select(\n",
    "    col(\"original_experience\"),\n",
    "    col(\"covered_months\")\n",
    ")\n",
    "\n",
    "# Step 6: Identify missing months per row\n",
    "# Aggregate the covered months per row\n",
    "covered_months_agg = covered_months.groupBy(\"original_experience\").agg(\n",
    "    collect_list(\"covered_months\").alias(\"all_covered_months\")\n",
    ")\n",
    "\n",
    "# Join with the full timeline and identify gaps\n",
    "result = global_dates.join(covered_months_agg, \"original_experience\").withColumn(\n",
    "    \"missing_months\",\n",
    "    array_except(f.array(col(\"full_months\")), col(\"all_covered_months\"))\n",
    ")\n",
    "\n",
    "# Show missing months per row\n",
    "result.select(\"original_experience\", \"missing_months\").display()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b80e20c7-2b34-49b2-86c5-e6fe4ee58bb2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import size, when\n",
    "\n",
    "# Add a column to check if there are missing months (binary)\n",
    "result = result.withColumn(\n",
    "    \"has_missing_months\",\n",
    "    when(size(col(\"missing_months\")) > 0, True).otherwise(False)\n",
    ")\n",
    "\n",
    "# Select the relevant columns\n",
    "result.select(\"original_experience\", \"has_missing_months\").display()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bee8be99-2a52-46a3-84d0-0b508a7696aa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, expr, lag, lead\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import ArrayType, StructType, StructField, StringType\n",
    "\n",
    "# Helper function to convert missing months to ranges\n",
    "def convert_to_ranges(df, column=\"missing_months\"):\n",
    "    # Explode the missing months array\n",
    "    exploded = df.select(\n",
    "        col(\"original_experience\"),\n",
    "        explode(col(column)).alias(\"missing_month\")\n",
    "    ).withColumn(\"missing_month\", to_date(expr(\"concat(missing_month, '-01')\"), \"yyyy-MM-dd\"))\n",
    "    \n",
    "    # Add lag and lead to identify ranges\n",
    "    window = Window.partitionBy(\"original_experience\").orderBy(\"missing_month\")\n",
    "    exploded = exploded.withColumn(\"prev_month\", lag(\"missing_month\").over(window))\n",
    "    exploded = exploded.withColumn(\"gap\", expr(\"missing_month - INTERVAL 1 MONTH != prev_month\"))\n",
    "    \n",
    "    # Assign groups to ranges\n",
    "    exploded = exploded.withColumn(\n",
    "        \"range_group\", expr(\"SUM(CASE WHEN gap THEN 1 ELSE 0 END) OVER (PARTITION BY original_experience ORDER BY missing_month)\")\n",
    "    )\n",
    "    \n",
    "    # Aggregate ranges\n",
    "    ranges = exploded.groupBy(\"original_experience\", \"range_group\").agg(\n",
    "        expr(\"MIN(missing_month)\").alias(\"range_start\"),\n",
    "        expr(\"MAX(missing_month)\").alias(\"range_end\")\n",
    "    ).select(\"original_experience\", \"range_start\", \"range_end\")\n",
    "    \n",
    "    return ranges\n",
    "\n",
    "# Apply the function\n",
    "missing_month_ranges = convert_to_ranges(result, \"missing_months\")\n",
    "\n",
    "# Show the ranges\n",
    "missing_month_ranges.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e01fdce6-5fbb-4c32-9923-0bf47b9b1f5e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import collect_list, struct\n",
    "\n",
    "# Combine ranges into an array of start-end pairs\n",
    "combined_ranges = missing_month_ranges.groupBy(\"original_experience\").agg(\n",
    "    collect_list(struct(\"range_start\", \"range_end\")).alias(\"missing_ranges\")\n",
    ")\n",
    "\n",
    "# Show the result\n",
    "combined_ranges.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e2cd949-8f3f-4e78-af22-8a0a1357e42e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, explode, expr, lag, struct, collect_list, to_date, when\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Step 1: Explode missing_months into individual rows\n",
    "exploded = profiles.withColumn(\"missing_month\", explode(col(\"missing_months\"))) \\\n",
    "             .withColumn(\"missing_month\", when(col(\"missing_month\").isNotNull(), \n",
    "                                               to_date(expr(\"concat(missing_month, '-01')\"), \"yyyy-MM-dd\"))\n",
    "                                    .otherwise(None))\n",
    "\n",
    "# Step 2: Identify gaps between consecutive months\n",
    "window = Window.partitionBy(\"original_experience\").orderBy(\"missing_month\")\n",
    "exploded = exploded.withColumn(\"prev_month\", lag(\"missing_month\").over(window))\n",
    "exploded = exploded.withColumn(\"gap\", expr(\"missing_month - INTERVAL 1 MONTH != prev_month\"))\n",
    "\n",
    "# Step 3: Assign groups for consecutive months\n",
    "exploded = exploded.withColumn(\n",
    "    \"range_group\",\n",
    "    expr(\"SUM(CASE WHEN gap THEN 1 ELSE 0 END) OVER (PARTITION BY original_experience ORDER BY missing_month)\")\n",
    ")\n",
    "\n",
    "# Step 4: Aggregate ranges\n",
    "ranges = exploded.groupBy(\"original_experience\", \"range_group\").agg(\n",
    "    expr(\"MIN(missing_month)\").alias(\"range_start\"),\n",
    "    expr(\"MAX(missing_month)\").alias(\"range_end\")\n",
    ")\n",
    "\n",
    "# Step 5: Exclude null ranges and group them into an array\n",
    "cleaned_ranges = ranges.filter(\n",
    "    col(\"range_start\").isNotNull() & col(\"range_end\").isNotNull()\n",
    ")\n",
    "\n",
    "combined_ranges = cleaned_ranges.groupBy(\"original_experience\").agg(\n",
    "    collect_list(struct(\"range_start\", \"range_end\")).alias(\"missing_ranges\")\n",
    ")\n",
    "\n",
    "# Step 6: Handle rows with no valid ranges (e.g., all nulls)\n",
    "final_result = combined_ranges.withColumn(\n",
    "    \"missing_ranges\",\n",
    "    when(col(\"missing_ranges\").isNotNull(), col(\"missing_ranges\")).otherwise(f.array())\n",
    ")\n",
    "\n",
    "# Show the final result\n",
    "final_result.display()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "plots",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
