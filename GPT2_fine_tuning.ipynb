{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "974c288b-e7b3-4219-b99b-3e7b2a7f8ed9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as f\n",
    "import pandas as pd\n",
    "from pyspark.ml.feature import CountVectorizer, Tokenizer, StringIndexer, VectorAssembler, Tokenizer, OneHotEncoder, Word2Vec, HashingTF, IndexToString\n",
    "from pyspark.ml.linalg import SparseVector, Vectors\n",
    "import numpy as np\n",
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "from pyspark.ml.classification import RandomForestClassifier, LogisticRegression\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.sql.window import Window\n",
    "from datetime import datetime\n",
    "import re\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "74d36f36-7683-4266-8840-b54c044d670f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# new df with scores\n",
    "profiles_with_scores = spark.read.parquet(\"/Workspace/Users/lihi.kaspi@campus.technion.ac.il/user_profiles_with_scores.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "90e638d3-90cf-4f04-989a-4c57bf0f8c5e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "profiles_with_scores = profiles_with_scores.withColumn(\n",
    "    'label', \n",
    "    f.when(f.col('profile_score') < 5, 0\n",
    "    ).when(f.col('profile_score') < 10, 1\n",
    "    ).when(f.col('profile_score') < 15, 2\n",
    "    ).when(f.col('profile_score') < 20, 3\n",
    "    ).otherwise(4)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1baf85d0-97a7-4f98-8fe7-bc3515a39b7f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Pre process good profile data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "03930dbc-3163-4e1b-b2c9-0aa5d07f689c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, concat_ws, udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "good_profiles_df = profiles_with_scores.filter(col('label').isin([3,4])).select(['id', 'city', 'education', 'name', 'position', 'about']).dropna()\n",
    "\n",
    "\n",
    "\n",
    "def strip_and_choose_first(str_lst):\n",
    "    return str_lst.strip(\"[]\").split(\", \")[0]\n",
    "\n",
    "\n",
    "# UDF to process the 'education' field (extract degree and school information)\n",
    "def process_education(degree, field, title):\n",
    "    # Extract degree, field, and school title from each education entry\n",
    "    degree = strip_and_choose_first(degree)\n",
    "    field = strip_and_choose_first(field)\n",
    "    title = strip_and_choose_first(title)\n",
    "    edu_details = f\"{degree} in {field} from {title}\"\n",
    "    return edu_details\n",
    "\n",
    "# Register UDF\n",
    "process_education_udf = udf(process_education, StringType())\n",
    "\n",
    "# Filter rows where the education column is not empty\n",
    "filtered_df = good_profiles_df.filter((col(\"education\").isNotNull()) & (col(\"education\") != f.lit([])))\n",
    "\n",
    "filtered_df = filtered_df.withColumn('degree', col('education').getField('degree').cast('string'))\n",
    "filtered_df = filtered_df.withColumn('field', col('education').getField('field').cast('string'))\n",
    "filtered_df = filtered_df.withColumn('school', col('education').getField('title').cast('string'))\n",
    "\n",
    "# Process the DataFrame\n",
    "good_profiles_df = filtered_df.withColumn(\"processed_education\", \n",
    "                                          process_education_udf(col('degree'), col('field'), col('school')))\n",
    "\n",
    "# Show the resulting DataFrame\n",
    "processed_df = good_profiles_df.withColumn(\n",
    "                                    \"input_prompt\",\n",
    "                                    concat_ws(\n",
    "                                        \", \",\n",
    "                                        col(\"city\"),\n",
    "                                        col(\"processed_education\"),\n",
    "                                        col(\"name\"),\n",
    "                                        col(\"position\"),\n",
    "                                    )\n",
    "                            )\n",
    "processed_df.display(limit=10)\n",
    "print(processed_df.count())\n",
    "processed_df.write.mode(\"overwrite\").parquet(\"/Workspace/Users/lihi.kaspi@campus.technion.ac.il/training_data.parquet\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de5f343d-b1d3-4aa0-85e0-70e6b2647fb4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Setting up dataset class, with stochastic sampling to account for size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e7592cbf-1e2b-4068-906e-1e3aea610de4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Initialize the tokenizer and SparkDataset\n",
    "from transformers import GPT2Tokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "\n",
    "\n",
    "#Util function\n",
    "def get_sampled_dataframe(df, sample_size=1000, seed=None):\n",
    "    \"\"\"\n",
    "    Randomly samples rows from a Spark DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): The full Spark DataFrame.\n",
    "        sample_size (int): The number of rows to sample.\n",
    "        seed (int, optional): Random seed for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: A sampled Spark DataFrame.\n",
    "    \"\"\"\n",
    "    return df.sample(withReplacement=False, fraction=sample_size / df.count())\n",
    "\n",
    "class SparkSampledDataset(torch.utils.data.IterableDataset):\n",
    "    def __init__(self, spark_df, tokenizer, max_length=512, sample_size=100):\n",
    "        \"\"\"\n",
    "        PyTorch Dataset that samples from a Spark DataFrame dynamically with a fixed number of batches.\n",
    "\n",
    "        Args:\n",
    "            spark_df (DataFrame): The full Spark DataFrame.\n",
    "            tokenizer: Hugging Face tokenizer.\n",
    "            max_length (int): Maximum sequence length for tokenization.\n",
    "            sample_size (int): Number of rows to sample at each iteration.\n",
    "            num_batches (int): Total number of batches to generate.\n",
    "            seed (int, optional): Random seed for reproducibility.\n",
    "        \"\"\"\n",
    "        self.spark_df = spark_df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.sample_size = sample_size\n",
    "\n",
    "    def __iter__(self):\n",
    "        \"\"\"\n",
    "        Yields batches of tokenized data sampled from the Spark DataFrame.\n",
    "        Stops after `num_batches` iterations.\n",
    "        \"\"\"\n",
    "        # Sample a subset of the DataFrame\n",
    "        sampled_df = get_sampled_dataframe(self.spark_df, sample_size=self.sample_size)\n",
    "        local_data = [row.asDict() for row in sampled_df.collect()]\n",
    "        # Tokenize each row in the sample\n",
    "        for row in local_data:\n",
    "            input_text = f'Here is some of my data:{row[\"input_prompt\"]}. Please craft an about section for me.'  \n",
    "            print(input_text)\n",
    "            # Replace with your input column name\n",
    "            target_text = row[\"about\"]  # Replace with your target column name\n",
    "            print(target_text)\n",
    "            # Tokenize inputs and targets\n",
    "            inputs = self.tokenizer(\n",
    "                input_text,\n",
    "                truncation=True,\n",
    "                padding=\"max_length\",\n",
    "                max_length=self.max_length,\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "            targets = self.tokenizer(\n",
    "                target_text,\n",
    "                truncation=True,\n",
    "                padding=\"max_length\",\n",
    "                max_length=self.max_length,\n",
    "                return_tensors=\"pt\"\n",
    "            )   \n",
    "            # Yield tokenized data\n",
    "            yield {\n",
    "                \"input_ids\": inputs[\"input_ids\"].squeeze(0),\n",
    "                \"attention_mask\": inputs[\"attention_mask\"].squeeze(0),\n",
    "                \"labels\": targets[\"input_ids\"].squeeze(0)\n",
    "            }\n",
    "\n",
    "file_path = \"/Workspace/Users/lihi.kaspi@campus.technion.ac.il/training_data.parquet\"\n",
    "\n",
    "# Read the Parquet file into a Spark DataFrame\n",
    "df = spark.read.parquet(file_path)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Set the padding token\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "dataset = SparkSampledDataset(\n",
    "    spark_df=df,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=256,\n",
    "    sample_size=100,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ab10c0a9-7fb9-4759-a5f9-d7640c6a86d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f5ba5cf-d5b0-4634-8350-53a5f1ca5ca7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create the DataLoader\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, AdamW\n",
    "import torch\n",
    "dataloader = DataLoader(dataset, batch_size=4)\n",
    "#---- Model definition ----\n",
    "# Load GPT-2 model and tokenizer\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Define pad_token (GPT-2 doesn't have a native pad_token)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Move the model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# ---- Optimizer definition ----\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "EPOCHS = 10\n",
    "\n",
    "# ---- Training loop ----\n",
    "model.train()\n",
    "for epoch in range(EPOCHS):  # Number of epochs\n",
    "    i=0\n",
    "    for batch in dataloader:\n",
    "        print(batch['input_ids'].shape, i)\n",
    "        i+=1\n",
    "        optimizer.zero_grad()  # Reset gradients\n",
    "        outputs = model(**batch)  # Forward pass\n",
    "        loss = outputs.loss      # Compute loss\n",
    "        loss.backward()          # Backward pass\n",
    "        optimizer.step()         # Update model parameters\n",
    "\n",
    "    print(f\"Epoch {epoch + 1} completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b74d526-8264-4808-ae84-7de8c8cc0ba7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read the Parquet file into a Spark DataFrame\n",
    "df = spark.read.parquet('/Workspace/Users/lihi.kaspi@campus.technion.ac.il/training_data.parquet')\n",
    "df.limit(10000).display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "48b6b469-0393-4174-8221-dc61f2424759",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.select(\"input_prompt\", \"about\").limit(10000).display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "724b7fad-54c7-411d-8ab4-464d7343a2d6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from transformers import BartForConditionalGeneration, BartTokenizer, AdamW\n",
    "from datasets import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ---- Model Definition ----\n",
    "model_name = \"facebook/bart-base\"  # Switch to bart-large for higher capacity\n",
    "tokenizer = BartTokenizer.from_pretrained(model_name, max_length=64)\n",
    "model = BartForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# ---- Dataset Preparation ----\n",
    "# Example data\n",
    "df = spark.read.parquet(\"/Workspace/Users/lihi.kaspi@campus.technion.ac.il/training_data.parquet\")\n",
    "data = df.limit(10000).collect()\n",
    "# Convert to Hugging Face Dataset format\n",
    "dataset = Dataset.from_dict({\n",
    "    \"input_text\": [d[\"input_prompt\"] for d in data],\n",
    "    \"output_text\": [d[\"about\"] for d in data]\n",
    "})\n",
    "\n",
    "# Tokenize the dataset\n",
    "def preprocess_function(examples):\n",
    "    inputs = tokenizer(\n",
    "        examples[\"input_text\"], max_length=512, truncation=True, padding=\"max_length\"\n",
    "    )\n",
    "    targets = tokenizer(\n",
    "        examples[\"output_text\"], max_length=512, truncation=True, padding=\"max_length\"\n",
    "    )\n",
    "    inputs[\"labels\"] = targets[\"input_ids\"]\n",
    "    return inputs\n",
    "\n",
    "tokenized_dataset = dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "# Create a DataLoader\n",
    "dataloader = DataLoader(tokenized_dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "# ---- Optimizer ----\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# ---- Training Loop ----\n",
    "EPOCHS = 5\n",
    "model.train()\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"Epoch {epoch + 1}/{EPOCHS}\")\n",
    "    epoch_loss = 0\n",
    "\n",
    "    for batch in tqdm(dataloader, desc=f\"Training Epoch {epoch + 1}\"):\n",
    "        # Move batch data to the same device as the model\n",
    "        input_ids = torch.stack([torch.tensor(ids) for ids in batch[\"input_ids\"]]).to(device)\n",
    "        attention_mask = torch.stack([torch.tensor(mask) for mask in batch[\"attention_mask\"]]).to(device)\n",
    "        labels = torch.stack([torch.tensor(label) for label in batch[\"labels\"]]).to(device)\n",
    "\n",
    "        # Zero out gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Track loss\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    # Log epoch loss\n",
    "    avg_loss = epoch_loss / len(dataloader)\n",
    "    print(f\"Epoch {epoch + 1} completed. Average Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    # Save the model and tokenizer after each epoch\n",
    "    model.save_pretrained(f\"/Workspace/Users/harel.oved@campus.technion.ac.il/bart_fine_tuned_epoch_{epoch + 1}\")\n",
    "    tokenizer.save_pretrained(f\"/Workspace/Users/harel.oved@campus.technion.ac.il/bart_fine_tuned_epoch_{epoch + 1}\")\n",
    "\n",
    "print(\"Training completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "88b3e490-b4ef-4173-a03b-3731c0682ea9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d61c5bf7-a8f8-4166-a71c-901db205acd5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41d39484-ef36-413a-a69e-d314c87af332",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Save the model and tokenizer\n",
    "# model.save_pretrained(\"Workspace/Users/lihi.kaspi@campus.technion.ac.il/fine_tuned_gpt2\")\n",
    "# tokenizer.save_pretrained(\"Workspace/Users/lihi.kaspi@campus.technion.ac.il/fine_tuned_gpt2\")\n",
    "# model.save_pretrained(\"dbfs:/Workspace/Users/lihi.kaspi@campus.technion.ac.il/fine_tuned_gpt2\")\n",
    "# tokenizer.save_pretrained(\"dbfs:/Workspace/Users/lihi.kaspi@campus.technion.ac.il/fine_tuned_gpt2\")\n",
    "\n",
    "# model.save_pretrained(\"/Workspace/Users/lihi.kaspi@campus.technion.ac.il/fine_tuned_GPT\")\n",
    "# tokenizer.save_pretrained(\"/Workspace/Users/lihi.kaspi@campus.technion.ac.il/fine_tuned_GPT\")\n",
    "# print(\"Model saved successfully!\")\n",
    "\n",
    "torch.save(model.state_dict(), \"/Workspace/Users/harel.oved@campus.technion.ac.il/gpt/model.pth\")\n",
    "# torch.save(tokenizer.state_dict(), \"/Workspace/Users/lihi.kaspi@campus.technion.ac.il/Profile-Pro/fine_tuned_GPT/tokenizer.pth\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9da99218-279b-4e3b-98c2-7368b09363a3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e897ad81-5fd8-4383-bcb7-f35bd1f24650",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, AdamW\n",
    "import torch\n",
    "\n",
    "\n",
    "model_path = \"/Workspace/Users/harel.oved@campus.technion.ac.il/gpt/model.pth\"\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Define pad_token (GPT-2 doesn't have a native pad_token)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4d309f94-013e-4ceb-969e-9af97a3ef6a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Dummy input\n",
    "input_prompt = \"Here is some of my details:New York, New York, United States, Master of Public Health - MPH in null from State University of New York (SUNY) Downstate Medical Center School of Public Health, Taylor A., Chief of Staff - MPH Candidate, please craft an about section\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "783bbebc-4dc9-46e5-a480-a2798493bb74",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Move model to GPU if available\n",
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Ensure the pad token is set (if applicable\n",
    "# )\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Tokenize the input\n",
    "inputs = tokenizer(\n",
    "    input_prompt,\n",
    "    return_tensors=\"pt\",\n",
    "    padding=True,           # Ensures padding if needed\n",
    "    truncation=True         # Ensures truncation to the model's max length\n",
    ").to(device)\n",
    "\n",
    "# Ensure `pad_token_id` is set\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Generate text\n",
    "output = model.generate(\n",
    "    inputs[\"input_ids\"],\n",
    "    attention_mask=inputs[\"attention_mask\"],  # Pass attention mask\n",
    "    max_length=100,       # Maximum length of the generated sequence\n",
    "    num_beams=5,          # Beam search for better results\n",
    "    early_stopping=True,  # Stop generation when reaching the EOS token\n",
    "    pad_token_id=tokenizer.pad_token_id       # Ensure correct padding\n",
    ")\n",
    "\n",
    "# Decode the generated text\n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"Generated Text:\", generated_text)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "GPT2_fine_tuning",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
