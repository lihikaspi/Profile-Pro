{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "974c288b-e7b3-4219-b99b-3e7b2a7f8ed9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as f\n",
    "import pandas as pd\n",
    "from pyspark.ml.feature import CountVectorizer, Tokenizer, StringIndexer, VectorAssembler, Tokenizer, OneHotEncoder, Word2Vec, HashingTF, IndexToString\n",
    "from pyspark.ml.linalg import SparseVector, Vectors\n",
    "import numpy as np\n",
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "from pyspark.ml.classification import RandomForestClassifier, LogisticRegression\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.sql.window import Window\n",
    "from datetime import datetime\n",
    "import re\n",
    "from pyspark.sql.functions import col, concat_ws, udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "74d36f36-7683-4266-8840-b54c044d670f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# new df with scores\n",
    "profiles_with_scores = spark.read.parquet(\"/Workspace/Users/lihi.kaspi@campus.technion.ac.il/user_profiles_with_scores.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "90e638d3-90cf-4f04-989a-4c57bf0f8c5e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "profiles_with_scores = profiles_with_scores.withColumn(\n",
    "    'label', \n",
    "    f.when(f.col('profile_score') < 5, 0\n",
    "    ).when(f.col('profile_score') < 10, 1\n",
    "    ).when(f.col('profile_score') < 15, 2\n",
    "    ).when(f.col('profile_score') < 20, 3\n",
    "    ).otherwise(4)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1baf85d0-97a7-4f98-8fe7-bc3515a39b7f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Pre process good profile data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "03930dbc-3163-4e1b-b2c9-0aa5d07f689c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "good_profiles_df = profiles_with_scores.filter(col('label').isin([3,4])).select(['id', 'city', 'education', 'name', 'position', 'about']).dropna()\n",
    "\n",
    "\n",
    "\n",
    "def strip_and_choose_first(str_lst):\n",
    "    return str_lst.strip(\"[]\").split(\", \")[0]\n",
    "\n",
    "\n",
    "# UDF to process the 'education' field (extract degree and school information)\n",
    "def process_education(degree, field, title):\n",
    "    # Extract degree, field, and school title from each education entry\n",
    "    degree = strip_and_choose_first(degree)\n",
    "    field = strip_and_choose_first(field)\n",
    "    title = strip_and_choose_first(title)\n",
    "    edu_details = f\"{degree} in {field} from {title}\"\n",
    "    return edu_details\n",
    "\n",
    "def process_df(df):\n",
    "    # Register UDF\n",
    "    process_education_udf = udf(process_education, StringType())\n",
    "\n",
    "    # Filter rows where the education column is not empty\n",
    "    filtered_df = df.filter((col(\"education\").isNotNull()) & (col(\"education\") != f.lit([])))\n",
    "\n",
    "    filtered_df = filtered_df.withColumn('degree', col('education').getField('degree').cast('string'))\n",
    "    filtered_df = filtered_df.withColumn('field', col('education').getField('field').cast('string'))\n",
    "    filtered_df = filtered_df.withColumn('school', col('education').getField('title').cast('string'))\n",
    "\n",
    "    # Process the DataFrame\n",
    "    good_profiles_df = filtered_df.withColumn(\"processed_education\", \n",
    "                                            process_education_udf(col('degree'), col('field'), col('school')))\n",
    "\n",
    "    # Show the resulting DataFrame\n",
    "    processed_df = good_profiles_df.withColumn(\n",
    "                                        \"input_prompt\",\n",
    "                                        concat_ws(\n",
    "                                            \", \",\n",
    "                                            col(\"city\"),\n",
    "                                            col(\"processed_education\"),\n",
    "                                            col(\"name\"),\n",
    "                                            col(\"position\"),\n",
    "                                        )\n",
    "                                )\n",
    "    processed_df.display(limit=10)\n",
    "    print(processed_df.count())\n",
    "    processed_df.write.mode(\"overwrite\").parquet(\"/Workspace/Users/lihi.kaspi@campus.technion.ac.il/training_data.parquet\")\n",
    "    return processed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e94bf32-54fb-47c6-9834-a815957f8a87",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "profiles_with_scores = spark.read.parquet(\"/Workspace/Users/lihi.kaspi@campus.technion.ac.il/user_profiles_with_scores.parquet\")\n",
    "profiles_with_scores.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c0b48f31-2beb-4023-b104-e83a699827b5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "profiles = spark.read.parquet('/dbfs/linkedin_people_train_data')\n",
    "import pyspark.sql.functions as f\n",
    "jobs = profiles.select('name', 'id', 'city', 'country_code', f.col('current_company').getField('name').alias('company_name'), f.col('experience')[0].getField('title').alias('job_title'), 'position')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a6ec0ba2-2190-457e-a12c-75cb802c2d27",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "def process_education(degree, field, title):\n",
    "    # Extract degree, field, and school title from each education entry\n",
    "    degree = strip_and_choose_first(degree)\n",
    "    field = strip_and_choose_first(field)\n",
    "    title = strip_and_choose_first(title)\n",
    "    edu_details = f\"{degree} in {field} from {title}\"\n",
    "    return edu_details\n",
    "\n",
    "\n",
    "profiles_with_scores = spark.read.parquet(\"/Workspace/Users/lihi.kaspi@campus.technion.ac.il/user_profiles_with_scores.parquet\")\n",
    "\n",
    "\n",
    "job_titles_df = jobs.select(\n",
    "    f.when(f.col('job_title').isNotNull(), f.lower(f.col('job_title')))\n",
    "    .otherwise(f.when(f.col('position').isNotNull(), f.lower(f.col('position'))).otherwise(f.lit(''))\n",
    "    .alias('processed_title')), 'id'\n",
    ")\n",
    "\n",
    "profiles_with_title = profiles_with_scores.join(job_titles_df, on='id')\n",
    "edu_filtered_df = profiles_with_title.filter((col(\"education\").isNotNull()) & (col(\"education\") != f.lit([])))\n",
    "no_edu_df = good_profiles_df.filter((col(\"education\").isNull()) | (col(\"education\") == f.lit([])))\n",
    "\n",
    "filtered_df = edu_filtered_df.withColumn('degree', col('education').getField('degree').cast('string'))\n",
    "filtered_df = filtered_df.withColumn('field', col('education').getField('field').cast('string'))\n",
    "filtered_df = filtered_df.withColumn('school', col('education').getField('title').cast('string'))\n",
    "\n",
    "# Process the DataFrame\n",
    "edu_filtered_df = filtered_df.withColumn(\"processed_education\", \n",
    "                                        process_education_udf(col('degree'), col('field'), col('school')))\n",
    "no_edu_df = no_edu_df.withColumn(\"processed_education\", lit(''))\n",
    "df = edu_filtered_df.union(no_edu_df)\n",
    "good_profiles_df = df.filter(col('label').isin([3,4])).select(['id', 'city', 'education', 'name', 'position', 'about', 'recommendations']).dropna()\n",
    "df.display()\n",
    "good_profiles_df.display()\n",
    "\n",
    "\n",
    "\n",
    "tokenizer_title = Tokenizer(inputCol=\"processed_title\", outputCol=\"tokened_title\")\n",
    "w2v_title = Word2Vec(inputCol=\"tokened_title\", outputCol=\"vector_title\", vectorSize=200, minCount=1)\n",
    "\n",
    "tokenizer_edu = Tokenizer(inputCol=\"processed_education\", outputCol=\"tokened_edu\")\n",
    "w2v_edu = Word2Vec(inputCol=\"tokened_edu\", outputCol=\"vector_edu\", vectorSize=200, minCount=1)\n",
    "\n",
    "# Build the pipeline\n",
    "pipeline = Pipeline(stages=[tokenizer_title, w2v_title, tokenizer_edu, w2v_edu])\n",
    "\n",
    "# Train the pipeline model\n",
    "model_vectorize = pipeline.fit(job_titles_df)\n",
    "\n",
    "# Create embeddings for job titles and centroids\n",
    "jobs_with_vectors = model_vectorize.transform(job_titles_df)\n",
    "centroids_with_vectors = model_vectorize.transform(centroids_df)\n",
    "\n",
    "jobs_temp = jobs_with_vectors.withColumnRenamed('vector', 'job_vector')\n",
    "jobs_temp = jobs_temp.withColumnRenamed('processed_title', 'job_title')\n",
    "\n",
    "centroids_temp = centroids_with_vectors.withColumnRenamed('processed_title', 'meta_job')\n",
    "centroids_temp = centroids_temp.withColumnRenamed('vector', 'centroid_vector')\n",
    "\n",
    "joined = jobs_temp.join(centroids_temp)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "GPT2_fine_tuning",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
