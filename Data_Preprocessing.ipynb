{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d227fa75-5b1f-451d-a2f7-698c3d84ab96",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import CountVectorizer, Tokenizer, StringIndexer, VectorAssembler, Tokenizer, OneHotEncoder, Word2Vec, HashingTF, IndexToString\n",
    "from pyspark.ml.linalg import SparseVector, Vectors\n",
    "import numpy as np\n",
    "import sparknlp\n",
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "from pyspark.ml.classification import RandomForestClassifier, LogisticRegression\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from sparknlp.base import DocumentAssembler, Finisher\n",
    "from sparknlp.annotator import Tokenizer, StopWordsCleaner, WordEmbeddingsModel, SentenceEmbeddings, BertEmbeddings, Word2VecModel\n",
    "from pyspark.ml.feature import VectorAssembler, OneHotEncoder, StringIndexer\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "from pyspark.sql.types import ArrayType, FloatType\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.ml.linalg import VectorUDT, DenseVector\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "import pyspark.sql.functions as f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "79860693-9d85-4346-924e-e06fffa089c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "profiles_with_scores = spark.read.parquet(\"/Workspace/Users/lihi.kaspi@campus.technion.ac.il/user_profiles_with_scores.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e254344a-f0ef-43b4-b1de-e1d32cf7dc73",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "profiles_with_scores = profiles_with_scores.withColumn(\"about\", f.when(f.col(\"about\").isNull(), \"No info\").otherwise(f.col(\"about\")))\n",
    "profiles_with_scores = profiles_with_scores.withColumn(\"position\", f.col(\"position\").cast(\"string\"))\n",
    "profiles_with_scores = profiles_with_scores.withColumn(\"position\", f.when(f.col(\"position\").isNull(), \"No info\").otherwise(f.col(\"position\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ccaff866-93a4-4be6-b9db-58b3f707a967",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sample = profiles_with_scores.select('profile_score').toPandas()\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.hist(sample['profile_score'], bins=50, edgecolor='k', alpha=0.7, color='goldenrod')\n",
    "plt.title('Histogram of Profile Scores Before Optimization')\n",
    "plt.xlabel('Profile Score')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d7cd735d-8b35-4be9-b2fc-b99d8283be99",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(profiles_with_scores.limit(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "763a2956-e944-429b-b05e-846b14d5bd8a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1. Preprocess `about` using Spark NLP\n",
    "document_assembler = DocumentAssembler() \\\n",
    "    .setInputCol(\"about\") \\\n",
    "    .setOutputCol(\"about_document\")\n",
    "\n",
    "tokenizer = Tokenizer() \\\n",
    "    .setInputCols([\"about_document\"]) \\\n",
    "    .setOutputCol(\"about_token\")\n",
    "\n",
    "stopwords_cleaner = StopWordsCleaner() \\\n",
    "    .setInputCols([\"about_token\"]) \\\n",
    "    .setOutputCol(\"about_clean_tokens\")\n",
    "\n",
    "embeddings = BertEmbeddings.pretrained(\"small_bert_L2_128\") \\\n",
    "    .setInputCols([\"about_document\", \"about_clean_tokens\"]) \\\n",
    "    .setOutputCol(\"about_embeddings_bert\")\n",
    "\n",
    "sentence_embeddings = SentenceEmbeddings() \\\n",
    "    .setInputCols([\"about_document\", \"about_embeddings_bert\"]) \\\n",
    "    .setOutputCol(\"about_embeddings\")\n",
    "\n",
    "nlp_pipeline_about = Pipeline(stages=[document_assembler, tokenizer, stopwords_cleaner, embeddings, sentence_embeddings])\n",
    "\n",
    "# Apply NLP Pipeline\n",
    "nlp_model_about = nlp_pipeline_about.fit(profiles_with_scores)\n",
    "processed_data1 = nlp_model_about.transform(profiles_with_scores)\n",
    "display(processed_data1.limit(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d7b5a8d6-c755-4b31-886e-501863fad25b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Check that 'position' exists in the original DataFrame (profiles_with_scores)\n",
    "profiles_with_scores.columns\n",
    "\n",
    "# 1. Preprocess `position` using Spark NLP\n",
    "document_assembler = DocumentAssembler() \\\n",
    "    .setInputCol(\"position\") \\\n",
    "    .setOutputCol(\"position_document\")\n",
    "\n",
    "tokenizer = Tokenizer() \\\n",
    "    .setInputCols([\"position_document\"]) \\\n",
    "    .setOutputCol(\"position_token\")\n",
    "\n",
    "stopwords_cleaner = StopWordsCleaner() \\\n",
    "    .setInputCols([\"position_token\"]) \\\n",
    "    .setOutputCol(\"position_clean_tokens\")\n",
    "\n",
    "embeddings = BertEmbeddings.pretrained(\"small_bert_L2_128\") \\\n",
    "    .setInputCols([\"position_document\", \"position_clean_tokens\"]) \\\n",
    "    .setOutputCol(\"position_embeddings_bert\")\n",
    "\n",
    "sentence_embeddings = SentenceEmbeddings() \\\n",
    "    .setInputCols([\"position_document\", \"position_embeddings_bert\"]) \\\n",
    "    .setOutputCol(\"position_embeddings\")\n",
    "\n",
    "# Define the NLP pipeline\n",
    "nlp_pipeline_position = Pipeline(stages=[document_assembler, tokenizer, stopwords_cleaner, embeddings, sentence_embeddings])\n",
    "\n",
    "# Apply NLP Pipeline to the original data\n",
    "nlp_model_position = nlp_pipeline_position.fit(profiles_with_scores)\n",
    "processed_data2 = nlp_model_position.transform(profiles_with_scores)\n",
    "\n",
    "# Debugging Step: Display columns of processed_data2\n",
    "processed_data2.columns\n",
    "\n",
    "# Debugging Step: Show the first few rows to check the content\n",
    "processed_data2.select('id', 'position_document').show(5)\n",
    "\n",
    "# Display the processed result\n",
    "display(processed_data2.limit(100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c7770876-e999-4683-a006-fe426cd1b334",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Select only the 'about' related columns from df1 (processed_data1)\n",
    "processed_data1_selected = processed_data1.select(\n",
    "    'id',  # Keep the 'id' column for the join\n",
    "    'about',  # Only select 'about' column\n",
    "    'about_document',\n",
    "    'about_token',\n",
    "    'about_clean_tokens',\n",
    "    'about_embeddings_bert',\n",
    "    'about_embeddings'\n",
    ")\n",
    "\n",
    "# Now perform the join, keeping all columns from df2 (processed_data2)\n",
    "processed_data = processed_data2.join(\n",
    "    processed_data1_selected,  # All columns from df2 will be kept\n",
    "    on=\"id\", \n",
    "    how=\"inner\"\n",
    ")\n",
    "\n",
    "# Display the result\n",
    "display(processed_data.limit(100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "843ca5d1-7f54-40c4-9a20-1bff740cd0c3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "processed_data1.select('id').show(5)\n",
    "processed_data2.select('id').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9a439e18-8f82-4e7e-8f97-54da925db755",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 3. Numerical Features\n",
    "processed_data = processed_data \\\n",
    "    .withColumn(\"num_education\", f.when(f.size(f.col('education')).isNull(), 0).otherwise(f.size(f.col('education')))) \\\n",
    "    .withColumn(\"num_experience\", f.when(f.size(f.col('experience')).isNull(), 0).otherwise(f.size(f.col('experience')))) \\\n",
    "    .withColumn(\"num_languages\", f.when(f.size(f.col('languages')).isNull(), 0).otherwise(f.size(f.col('languages')))) \\\n",
    "    .withColumn(\"total_followers\", f.when(f.col(\"followers\").isNull(), 0).otherwise(f.col(\"followers\"))) \\\n",
    "    .withColumn(\"num_recommendations\", f.when(f.col(\"recommendations_count\").isNull(), 0).otherwise(f.col(\"recommendations_count\")))\n",
    "\n",
    "display(processed_data.limit(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "30e732cb-bf01-4516-9048-49fe1f0982dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def to_dense_vector(embeddings_array):\n",
    "    return Vectors.dense(embeddings_array)\n",
    "\n",
    "# Register a UDF to convert arrays to dense vectors\n",
    "to_dense_udf = udf(lambda x: to_dense_vector(x), VectorUDT())\n",
    "\n",
    "# Apply the UDF to the embeddings column (adjust column name as needed)\n",
    "processed_data = processed_data.withColumn(\n",
    "    \"about_embeddings_dense\", \n",
    "    to_dense_udf(f.expr(\"about_embeddings.embeddings[0]\"))\n",
    ")\n",
    "processed_data = processed_data.withColumn(\n",
    "    \"position_embeddings_dense\", \n",
    "    to_dense_udf(f.expr(\"position_embeddings.embeddings[0]\"))\n",
    ")\n",
    "\n",
    "# Assemble features\n",
    "assembler = VectorAssembler(inputCols=[\n",
    "    \"about_embeddings_dense\", \"position_embeddings_dense\", \"num_education\", \"num_experience\", \"num_languages\",\n",
    "    \"total_followers\", \"num_recommendations\",\n",
    "], outputCol=\"features\")\n",
    "\n",
    "final_data = assembler.transform(processed_data)\n",
    "\n",
    "# Select relevant columns\n",
    "final_data = final_data.select('id' , \"features\", \"profile_score\")\n",
    "\n",
    "display(final_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8037c4cd-3531-44ff-bf79-959c295eb7fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# final_data.write.parquet(\"processed_profile_data.parquet\")\n",
    "final_data.select('id', 'features', 'profile_score').write.mode(\"overwrite\").parquet(\"/Workspace/Users/lihi.kaspi@campus.technion.ac.il/processed_data.parquet\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Data_Preprocessing",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
