{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "974c288b-e7b3-4219-b99b-3e7b2a7f8ed9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as f\n",
    "import pandas as pd\n",
    "from pyspark.ml.feature import CountVectorizer, Tokenizer as Tokenizer_feature, StringIndexer, VectorAssembler, OneHotEncoder, Word2Vec, HashingTF, IndexToString\n",
    "from pyspark.ml.linalg import SparseVector, Vectors\n",
    "import numpy as np\n",
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "from pyspark.ml.classification import RandomForestClassifier, LogisticRegression\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.sql.window import Window\n",
    "from datetime import datetime\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark.sql.functions import col, concat_ws, udf, lit\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import broadcast\n",
    "from pyspark.ml.functions import vector_to_array\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number\n",
    "from huggingface_hub import InferenceClient\n",
    "from huggingface_hub import login\n",
    "import time\n",
    "from pyspark.ml.linalg import VectorUDT, DenseVector\n",
    "import numpy as np\n",
    "import sparknlp\n",
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "from pyspark.sql.types import ArrayType, FloatType\n",
    "from pyspark.sql.functions import udf\n",
    "from sparknlp.base import DocumentAssembler, Finisher\n",
    "from sparknlp.annotator import Tokenizer, StopWordsCleaner, WordEmbeddingsModel, SentenceEmbeddings, BertEmbeddings, Word2VecModel\n",
    "from pyspark.ml.classification import MultilayerPerceptronClassificationModel\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1baf85d0-97a7-4f98-8fe7-bc3515a39b7f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Pre process good profile data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a6ec0ba2-2190-457e-a12c-75cb802c2d27",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def strip_and_choose_first(str_lst):\n",
    "    return str_lst.strip(\"[]\").split(\", \")[0]\n",
    "\n",
    "\n",
    "def process_education(degree, field, title):\n",
    "    # Extract degree, field, and school title from each education entry\n",
    "    degree = strip_and_choose_first(degree)\n",
    "    field = strip_and_choose_first(field)\n",
    "    title = strip_and_choose_first(title)\n",
    "    edu_details = f\"{degree} in {field} from {title}\"\n",
    "    return edu_details\n",
    "\n",
    "\n",
    "def preprocess_profiles(df):\n",
    "    \"\"\"\n",
    "    df: profiles dataframe, such that name, id, city, country_code, experience, position are in the correct format\n",
    "    returns pre processed dataframe.\n",
    "    \"\"\"\n",
    "    jobs = df.select('name', 'id', 'city', 'country_code', f.col('experience')[0].getField('title').alias('job_title'), 'position')\n",
    "    process_education_udf = udf(process_education, StringType())\n",
    "    job_titles_df = jobs.select(\n",
    "        f.when(f.col('job_title').isNotNull(), f.lower(f.col('job_title')))\n",
    "        .otherwise(f.when(f.col('position').isNotNull(), f.lower(f.col('position'))).otherwise(f.lit('')))\n",
    "        .alias('processed_title'), 'id'\n",
    "    )\n",
    "\n",
    "    df = df.join(job_titles_df, on='id')\n",
    "    edu_filtered_df = df.filter((col(\"education\").isNotNull()) & (col(\"education\") != f.lit([])))\n",
    "    no_edu_df = df.filter((col(\"education\").isNull()) | (col(\"education\") == f.lit([])))\n",
    "\n",
    "    filtered_df = edu_filtered_df.withColumn('degree', col('education').getField('degree').cast('string'))\n",
    "    filtered_df = filtered_df.withColumn('field', col('education').getField('field').cast('string'))\n",
    "    filtered_df = filtered_df.withColumn('school', col('education').getField('title').cast('string'))\n",
    "\n",
    "    # Process the DataFrame\n",
    "    edu_filtered_df = filtered_df.withColumn(\"processed_education\", \n",
    "                                            process_education_udf(col('degree'), col('field'), col('school')))\n",
    "    no_edu_df = no_edu_df.withColumn(\"processed_education\", lit(''))\n",
    "    edu_filtered_df = edu_filtered_df.select(['id', 'processed_education', 'processed_title', 'name','city'])\n",
    "    no_edu_df = no_edu_df.select(['id', 'processed_education', 'processed_title', 'name','city'])\n",
    "    df = edu_filtered_df.union(no_edu_df)\n",
    "    return df \n",
    "\n",
    "def generate_small_good_sample():\n",
    "    profiles_with_scores = spark.read.parquet(\"/Workspace/Users/lihi.kaspi@campus.technion.ac.il/user_profiles_with_scores.parquet\")\n",
    "    profiles = spark.read.parquet('/dbfs/linkedin_people_train_data')\n",
    "    profiles_with_scores = profiles_with_scores.withColumn(\n",
    "        'label', \n",
    "        f.when(f.col('profile_score') < 5, 0\n",
    "        ).when(f.col('profile_score') < 10, 1\n",
    "        ).when(f.col('profile_score') < 15, 2\n",
    "        ).when(f.col('profile_score') < 20, 3\n",
    "        ).otherwise(4)\n",
    "    )\n",
    "    df = preprocess_profiles(profiles_with_scores)\n",
    "    df = df.join(profiles_with_scores, on='id')\n",
    "    good_profiles_df = df.filter(col('label').isin([3,4])).select(['id','processed_education','processed_title', 'about'])\n",
    "    good_profiles_df = good_profiles_df.limit(10000)\n",
    "    good_profiles_df.write.mode(\"overwrite\").parquet(\"/Workspace/Users/lihi.kaspi@campus.technion.ac.il/sample_good_profile_data.parquet\")\n",
    "generate_small_good_sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "94e6ed38-d3e6-4ee1-870a-b1702c51cae5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def df_to_vector(df, good_profiles_df):\n",
    "    tokenizer_title = Tokenizer_feature(inputCol=\"processed_title\", outputCol=\"tokened_title\")\n",
    "    w2v_title = Word2Vec(inputCol=\"tokened_title\", outputCol=\"vector_title\", vectorSize=200, minCount=1)\n",
    "\n",
    "    tokenizer_edu = Tokenizer_feature(inputCol=\"processed_education\", outputCol=\"tokened_edu\")\n",
    "    w2v_edu = Word2Vec(inputCol=\"tokened_edu\", outputCol=\"vector_edu\", vectorSize=200, minCount=1)\n",
    "\n",
    "    pipeline = Pipeline(stages=[tokenizer_title, w2v_title, tokenizer_edu, w2v_edu])\n",
    "\n",
    "    model_vectorize = pipeline.fit(df)\n",
    "\n",
    "    # Create embeddings for job titles and centroids\n",
    "    df_with_vectors = model_vectorize.transform(df)\n",
    "    good_with_vectors = model_vectorize.transform(good_profiles_df)\n",
    "    return df_with_vectors, good_with_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9460ab1f-30f4-4f12-b0bb-f9703bae4614",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def cross_dfs(df_with_vectors, good_with_vectors):\n",
    "    profiles = df_with_vectors.withColumnRenamed(\"vector_title\", \"pos_embed\") \\\n",
    "                            .withColumnRenamed(\"vector_edu\", \"edu_embed\") \\\n",
    "                            .withColumnRenamed(\"id\", \"profiles_id\")\n",
    "\n",
    "    good_profiles = good_with_vectors.withColumnRenamed(\"vector_title\", \"pos_embed_good\") \\\n",
    "                                    .withColumnRenamed(\"vector_edu\", \"edu_embed_good\") \\\n",
    "                                    .withColumnRenamed(\"id\", \"good_profile_id\")\n",
    "\n",
    "    good_profiles = good_profiles.select([\"good_profile_id\", \"pos_embed_good\", \"edu_embed_good\"])\n",
    "    profiles = profiles.select([\"profiles_id\", \"pos_embed\", \"edu_embed\"])\n",
    "\n",
    "    profiles = profiles.withColumn(\"edu_embed\", vector_to_array(col(\"edu_embed\")))\n",
    "    profiles = profiles.withColumn(\"pos_embed\", vector_to_array(col(\"pos_embed\")))\n",
    "\n",
    "    good_profiles = good_profiles.withColumn(\"edu_embed_good\", vector_to_array(col(\"edu_embed_good\")))\n",
    "    good_profiles = good_profiles.withColumn(\"pos_embed_good\", vector_to_array(col(\"pos_embed_good\")))\n",
    "\n",
    "    good_profiles_broadcast = broadcast(good_profiles)\n",
    "\n",
    "    profiles_cross = profiles.join(good_profiles_broadcast, how=\"inner\")\n",
    "    return profiles_cross"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d637bce-2b1e-4050-93bc-a6c46c9ab79e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def dot_product(vec1, vec2):\n",
    "    return F.expr(f\"\"\"\n",
    "        aggregate(transform({vec1}, (x, i) -> x * {vec2}[i]), 0D, (acc, x) -> acc + x)\n",
    "    \"\"\")\n",
    "\n",
    "def vector_norm(vec):\n",
    "    return F.sqrt(F.expr(f\"aggregate(transform({vec}, x -> x * x), 0D, (acc, x) -> acc + x)\"))\n",
    "def compute_sim(profiles_cross):\n",
    "    edu_dot_product = dot_product(\"edu_embed\", \"edu_embed_good\")\n",
    "    pos_dot_product = dot_product(\"pos_embed\", \"pos_embed_good\")\n",
    "\n",
    "\n",
    "    edu_norm_profile = vector_norm(\"edu_embed\")\n",
    "    edu_norm_good = vector_norm(\"edu_embed_good\")\n",
    "\n",
    "    pos_norm_profile = vector_norm(\"pos_embed\")\n",
    "    pos_norm_good = vector_norm(\"pos_embed_good\")\n",
    "\n",
    "    profiles_cross = profiles_cross.withColumn(\n",
    "        \"edu_sim\", edu_dot_product / (edu_norm_profile * edu_norm_good)\n",
    "    ).withColumn(\n",
    "        \"pos_sim\", pos_dot_product / (pos_norm_profile * pos_norm_good)\n",
    "    ).withColumn(\n",
    "        \"total_sim\", F.col(\"edu_sim\") + F.col(\"pos_sim\")\n",
    "    )\n",
    "    return profiles_cross"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "10a98909-bff8-45ec-b8d3-24bc639f87a3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_best_matches(profiles_cross):\n",
    "    # order by highest similarity\n",
    "    window_spec = Window.partitionBy(\"profiles_id\").orderBy(col(\"total_sim\").desc())\n",
    "    # Rank the matches and filter to keep only the best match per profile\n",
    "    best_matches = profiles_cross.withColumn(\"rank\", row_number().over(window_spec)).filter(col(\"rank\") == 1)\n",
    "\n",
    "    best_matches = best_matches.select(\n",
    "        col(\"profiles_id\"),\n",
    "        col(\"good_profile_id\").alias(\"matched_good_profile_id\"),\n",
    "        col(\"total_sim\")\n",
    "    )\n",
    "    return best_matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eecd68e2-77c4-4dc1-9117-6199c003f994",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_match_df(best_matches):\n",
    "    profiles_with_scores = spark.read.parquet(\"/Workspace/Users/lihi.kaspi@campus.technion.ac.il/user_profiles_with_scores.parquet\")\n",
    "    profiles_with_scores = profiles_with_scores.withColumn(\n",
    "        'label', \n",
    "        f.when(f.col('profile_score') < 5, 0\n",
    "        ).when(f.col('profile_score') < 10, 1\n",
    "        ).when(f.col('profile_score') < 15, 2\n",
    "        ).when(f.col('profile_score') < 20, 3\n",
    "        ).otherwise(4)\n",
    "    )\n",
    "    good_profiles = profiles_with_scores.filter(col('label').isin([3,4])).select(['id','about']).dropna().withColumnRenamed('id', \"matched_good_profile_id\")\n",
    "\n",
    "    match_df = best_matches.join(good_profiles, on=\"matched_good_profile_id\")\n",
    "    return match_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7752e023-7e67-4d64-a4bd-08b07bf8b745",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def generate_sections(bad_profile_df, match_df):\n",
    "    access_token = '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'\n",
    "    login(access_token)\n",
    "\n",
    "\n",
    "    df = match_df.withColumnRenamed('profiles_id', 'id').join(bad_profile_df.withColumnRenamed('some_column_name', 'id'), on=\"id\")\n",
    "    pd_df = df.toPandas()\n",
    "    \n",
    "    def create_section(user_data, procesed_edu, city, name, proccesed_title):\n",
    "        client = InferenceClient(token=access_token)\n",
    "        input_prompt = f\"This is an about section of a user similar to me:{user_data}. build an about section for me. my name is {name}, I live in {city}. my education details are {procesed_edu} and my job title is {proccesed_title}.  Do not use things like [Assuming a similar role as Fleet Account Manager based on Josh's profession] Business Development Specialist at [Assuming a company similar to Knapheide Manufacturing], it should look like a real about section\"\n",
    "        completion = client.text_generation(\n",
    "            model=\"mistralai/Mistral-7B-Instruct-v0.3\", \n",
    "            prompt=input_prompt, \n",
    "            max_new_tokens=500\n",
    "        )\n",
    "        return completion\n",
    "    i = 0\n",
    "    abouts = []\n",
    "    for _, row in pd_df.iterrows():\n",
    "        print(i)\n",
    "        i+=1\n",
    "        user_data = row['about']\n",
    "        name  = row['name']\n",
    "        city = row['city']\n",
    "        proccesed_edu = row['processed_education']\n",
    "        proccesed_title = row['processed_title']\n",
    "        completion = create_section(user_data, proccesed_edu, city, name, proccesed_title)\n",
    "        time.sleep(2)\n",
    "        abouts.append((row[\"id\"],completion))\n",
    "        if i == 400:\n",
    "            break\n",
    "    generated_abouts_df = spark.createDataFrame(abouts, [\"id\", \"about\"])\n",
    "\n",
    "    return generated_abouts_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef078095-9c29-4577-9607-52895f82f124",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def optimize_profiles(df,good_profiles=None):\n",
    "    \"\"\"\n",
    "    Optimize the profiles based on the good profiles provided.\n",
    "    If no good_profiles are provided, default to using the sample good profiles we defined.\n",
    "    \"\"\"\n",
    "    if good_profiles is None:\n",
    "        good_profiles = spark.read.parquet(\"/Workspace/Users/lihi.kaspi@campus.technion.ac.il/sample_good_profile_data.parquet\")\n",
    "    df = preprocess_profiles(df)\n",
    "    df_vector, good_profiles_vector = df_to_vector(df, good_profiles)\n",
    "    profiles_cross = cross_dfs(df_vector, good_profiles_vector)\n",
    "    sim_cross = compute_sim(profiles_cross)\n",
    "    best_matches = get_best_matches(sim_cross)\n",
    "    match_df = get_match_df(best_matches)\n",
    "    gen_df = generate_sections(df, match_df)\n",
    "    return gen_df\n",
    "profiles = spark.read.parquet('/dbfs/linkedin_people_train_data')\n",
    "test_df = profiles.limit(10)\n",
    "test_df.display()   \n",
    "gen_about_df = optimize_profiles(test_df)\n",
    "gen_about_df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f26a0c6b-2385-455b-9cf9-7bfaf1b969c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "gen_about_df = gen_about_df.withColumnRenamed('about', 'about_after')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "39a30576-18ba-4c78-bd2c-36894edac022",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "profiles = spark.read.parquet('/dbfs/linkedin_people_train_data')\n",
    "profiles = gen_about_df.join(profiles, on=\"id\")\n",
    "profiles = profiles.withColumn(\"about_position_after\", f.concat_ws(\" \", f.col(\"about_after\"), f.col(\"position\")))\n",
    "profiles = profiles.withColumn(\"about_position_before\", f.concat_ws(\" \", f.col(\"about\"), f.col(\"position\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f9646067-9d27-47f2-8f77-ef3ffb68cdcb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Preprocess `about` using Spark NLP\n",
    "document_assembler = DocumentAssembler() \\\n",
    "    .setInputCol(\"about_position_after\") \\\n",
    "    .setOutputCol(\"ap_document\")\n",
    "\n",
    "tokenizer = Tokenizer() \\\n",
    "    .setInputCols([\"ap_document\"]) \\\n",
    "    .setOutputCol(\"ap_token\")\n",
    "\n",
    "stopwords_cleaner = StopWordsCleaner() \\\n",
    "    .setInputCols([\"ap_token\"]) \\\n",
    "    .setOutputCol(\"ap_clean_tokens\")\n",
    "\n",
    "embeddings = BertEmbeddings.pretrained(\"small_bert_L2_128\") \\\n",
    "    .setInputCols([\"ap_document\", \"ap_clean_tokens\"]) \\\n",
    "    .setOutputCol(\"ap_embeddings_bert\")\n",
    "\n",
    "sentence_embeddings = SentenceEmbeddings() \\\n",
    "    .setInputCols([\"ap_document\", \"ap_embeddings_bert\"]) \\\n",
    "    .setOutputCol(\"about_position_embeddings\")\n",
    "\n",
    "nlp_pipeline_about = Pipeline(stages=[document_assembler, tokenizer, stopwords_cleaner, embeddings, sentence_embeddings])\n",
    "\n",
    "# Apply NLP Pipeline\n",
    "nlp_model_about = nlp_pipeline_about.fit(profiles)\n",
    "processed_data = nlp_model_about.transform(profiles)\n",
    "display(processed_data.limit(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "161b65b5-9b39-41f9-9c4e-1b8f858151b9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "model_path = 'dbfs:/Workspace/Users/lihi.kaspi@campus.technion.ac.il/mlp_model'\n",
    "model = MultilayerPerceptronClassificationModel.load(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec4cba14-eb6f-4a35-8ac0-f907e484d640",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Numerical Features\n",
    "processed_data = processed_data \\\n",
    "    .withColumn(\"num_education\", f.when(f.size(f.col('education')).isNull(), 0).otherwise(f.size(f.col('education')))) \\\n",
    "    .withColumn(\"num_experience\", f.when(f.size(f.col('experience')).isNull(), 0).otherwise(f.size(f.col('experience')))) \\\n",
    "    .withColumn(\"num_languages\", f.when(f.size(f.col('languages')).isNull(), 0).otherwise(f.size(f.col('languages')))) \\\n",
    "    .withColumn(\"total_followers\", f.when(f.col(\"followers\").isNull(), 0).otherwise(f.col(\"followers\"))) \\\n",
    "    .withColumn(\"num_recommendations\", f.when(f.col(\"recommendations_count\").isNull(), 0).otherwise(f.col(\"recommendations_count\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c0de5f29-d4b2-4f0a-a24c-b15563bcce9f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def to_dense_vector(embeddings_array):\n",
    "    return Vectors.dense(embeddings_array)\n",
    "\n",
    "to_dense_udf = udf(lambda x: to_dense_vector(x), VectorUDT())\n",
    "\n",
    "processed_data = processed_data.withColumn(\n",
    "    \"about_position_embeddings_dense\", \n",
    "    to_dense_udf(f.expr(\"about_position_embeddings.embeddings[0]\"))\n",
    ")\n",
    "\n",
    "# Assemble features\n",
    "assembler = VectorAssembler(inputCols=[\n",
    "    \"about_position_embeddings_dense\", \"num_education\", \"num_experience\", \"num_languages\",\n",
    "    \"total_followers\", \"num_recommendations\",\n",
    "], outputCol=\"features\")\n",
    "\n",
    "final_data = assembler.transform(processed_data)\n",
    "\n",
    "final_data = final_data.select('id', \"features\")\n",
    "\n",
    "display(final_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf443319-6443-44b7-8a7a-7453afab9f3b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mlp_predictions = model.transform(final_data)\n",
    "mlp_predictions = mlp_predictions.withColumn('prediction', f.when(f.col(\"prediction\") < 2, 2).otherwise(f.col(\"prediction\")))\\\n",
    "    .withColumn('initial_prediction', f.lit(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf768e8e-9e5e-4e70-a315-8e684f31d428",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sample = mlp_predictions.select('prediction', 'initial_prediction').toPandas()\n",
    "\n",
    "category_mapping = {0: 'bad', 1: 'below average', 2: 'average', 3: 'above average', 4: 'good'}\n",
    "\n",
    "sample['prediction_category'] = sample['prediction'].map(category_mapping)\n",
    "sample['initial_prediction_category'] = sample['initial_prediction'].map(category_mapping)\n",
    "\n",
    "initial_category_counts = sample['initial_prediction_category'].value_counts().reindex(category_mapping.values(), fill_value=0)\n",
    "pred_category_counts = sample['prediction_category'].value_counts().reindex(category_mapping.values(), fill_value=0)\n",
    "x = np.arange(len(category_mapping))\n",
    "bar_width = 0.4\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.bar(x - bar_width/2, initial_category_counts.values, color='#a2d5f2', width=bar_width, label='Before', edgecolor='black')\n",
    "plt.bar(x + bar_width/2, pred_category_counts.values, color='#f2aac7', width=bar_width, label='After', edgecolor='black')\n",
    "plt.title('Histogram of Profile Scores Before and After Optimization')\n",
    "plt.xlabel('Profile Score')\n",
    "plt.ylabel('Frequency')\n",
    "plt.xticks(x, category_mapping.values())\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3d066a5e-45d4-4b2a-b13a-2cef305df475",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data_with_pred = processed_data.join(mlp_predictions, on='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "20e84f0d-8489-4c5e-bf1e-d16a4c1daf0d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(data_with_pred.select(\"id\", 'about', 'about_after', 'prediction'))"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "LLM_promprt_engineering",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
