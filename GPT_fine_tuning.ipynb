{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "974c288b-e7b3-4219-b99b-3e7b2a7f8ed9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as f\n",
    "import pandas as pd\n",
    "from pyspark.ml.feature import CountVectorizer, Tokenizer, StringIndexer, VectorAssembler, Tokenizer, OneHotEncoder, Word2Vec, HashingTF, IndexToString\n",
    "from pyspark.ml.linalg import SparseVector, Vectors\n",
    "import numpy as np\n",
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "from pyspark.ml.classification import RandomForestClassifier, LogisticRegression\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.sql.window import Window\n",
    "from datetime import datetime\n",
    "import re\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1868b30e-5ba8-4ea8-87cb-23a980ebbe81",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "profiles = spark.read.parquet('/dbfs/linkedin_people_train_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "74d36f36-7683-4266-8840-b54c044d670f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# new df with scores\n",
    "profiles_with_scores = spark.read.parquet(\"/Workspace/Users/lihi.kaspi@campus.technion.ac.il/user_profiles_with_scores.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "90e638d3-90cf-4f04-989a-4c57bf0f8c5e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "profiles_with_scores = profiles_with_scores.withColumn(\n",
    "    'label', \n",
    "    f.when(f.col('profile_score') < 5, 0\n",
    "    ).when(f.col('profile_score') < 10, 1\n",
    "    ).when(f.col('profile_score') < 15, 2\n",
    "    ).when(f.col('profile_score') < 20, 3\n",
    "    ).otherwise(4)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1baf85d0-97a7-4f98-8fe7-bc3515a39b7f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Pre process good profile data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "03930dbc-3163-4e1b-b2c9-0aa5d07f689c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, concat_ws, udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "good_profiles_df = profiles_with_scores.filter(col('label').isin([3,4])).select(['city', 'education', 'name', 'position', 'about']).dropna()\n",
    "\n",
    "\n",
    "\n",
    "def strip_and_choose_first(str_lst):\n",
    "    return str_lst.strip(\"[]\").split(\", \")[0]\n",
    "\n",
    "\n",
    "# UDF to process the 'education' field (extract degree and school information)\n",
    "def process_education(degree, field, title):\n",
    "    # Extract degree, field, and school title from each education entry\n",
    "    degree = strip_and_choose_first(degree)\n",
    "    field = strip_and_choose_first(field)\n",
    "    title = strip_and_choose_first(title)\n",
    "    edu_details = f\"{degree} in {field} from {title}\"\n",
    "    return edu_details\n",
    "\n",
    "# Register UDF\n",
    "process_education_udf = udf(process_education, StringType())\n",
    "\n",
    "# Filter rows where the education column is not empty\n",
    "filtered_df = good_profiles_df.filter((col(\"education\").isNotNull()) & (col(\"education\") != f.lit([])))\n",
    "\n",
    "filtered_df = filtered_df.withColumn('degree', col('education').getField('degree').cast('string'))\n",
    "filtered_df = filtered_df.withColumn('field', col('education').getField('field').cast('string'))\n",
    "filtered_df = filtered_df.withColumn('school', col('education').getField('title').cast('string'))\n",
    "\n",
    "# Process the DataFrame\n",
    "good_profiles_df = filtered_df.withColumn(\"processed_education\", \n",
    "                                          process_education_udf(col('degree'), col('field'), col('school')))\n",
    "\n",
    "# Show the resulting DataFrame\n",
    "processed_df = good_profiles_df.withColumn(\n",
    "                                    \"input_prompt\",\n",
    "                                    concat_ws(\n",
    "                                        \", \",\n",
    "                                        col(\"city\"),\n",
    "                                        col(\"processed_education\"),\n",
    "                                        col(\"name\"),\n",
    "                                        col(\"position\"),\n",
    "                                    )\n",
    "                            )\n",
    "processed_df.display(limit=10)\n",
    "print(processed_df.count())\n",
    "processed_df.write.mode(\"overwrite\").parquet(\"/Workspace/Users/lihi.kaspi@campus.technion.ac.il/training_data.parquet\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "de5f343d-b1d3-4aa0-85e0-70e6b2647fb4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Setting up dataset class, with stochastic sampling to account for size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e7592cbf-1e2b-4068-906e-1e3aea610de4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Initialize the tokenizer and SparkDataset\n",
    "from transformers import GPT2Tokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "\n",
    "\n",
    "#Util function\n",
    "def get_sampled_dataframe(df, sample_size=1000, seed=None):\n",
    "    \"\"\"\n",
    "    Randomly samples rows from a Spark DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): The full Spark DataFrame.\n",
    "        sample_size (int): The number of rows to sample.\n",
    "        seed (int, optional): Random seed for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: A sampled Spark DataFrame.\n",
    "    \"\"\"\n",
    "    return df.sample(withReplacement=False, fraction=sample_size / df.count(), seed=seed)\n",
    "\n",
    "class SparkSampledDataset(torch.utils.data.IterableDataset):\n",
    "    def __init__(self, spark_df, tokenizer, max_length=512, sample_size=100, seed=None):\n",
    "        \"\"\"\n",
    "        PyTorch Dataset that samples from a Spark DataFrame dynamically with a fixed number of batches.\n",
    "\n",
    "        Args:\n",
    "            spark_df (DataFrame): The full Spark DataFrame.\n",
    "            tokenizer: Hugging Face tokenizer.\n",
    "            max_length (int): Maximum sequence length for tokenization.\n",
    "            sample_size (int): Number of rows to sample at each iteration.\n",
    "            num_batches (int): Total number of batches to generate.\n",
    "            seed (int, optional): Random seed for reproducibility.\n",
    "        \"\"\"\n",
    "        self.spark_df = spark_df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.sample_size = sample_size\n",
    "        self.seed = seed\n",
    "\n",
    "    def __iter__(self):\n",
    "        \"\"\"\n",
    "        Yields batches of tokenized data sampled from the Spark DataFrame.\n",
    "        Stops after `num_batches` iterations.\n",
    "        \"\"\"\n",
    "        # Sample a subset of the DataFrame\n",
    "        sampled_df = get_sampled_dataframe(self.spark_df, sample_size=self.sample_size, seed=self.seed)\n",
    "        local_data = [row.asDict() for row in sampled_df.collect()]\n",
    "        \n",
    "        # Tokenize each row in the sample\n",
    "        for row in local_data:\n",
    "            input_text = row[\"input_prompt\"]  # Replace with your input column name\n",
    "            target_text = row[\"about\"]  # Replace with your target column name\n",
    "            \n",
    "            # Tokenize inputs and targets\n",
    "            inputs = self.tokenizer(\n",
    "                input_text,\n",
    "                truncation=True,\n",
    "                padding=\"max_length\",\n",
    "                max_length=self.max_length,\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "            targets = self.tokenizer(\n",
    "                target_text,\n",
    "                truncation=True,\n",
    "                padding=\"max_length\",\n",
    "                max_length=self.max_length,\n",
    "                return_tensors=\"pt\"\n",
    "            )   \n",
    "            # Yield tokenized data\n",
    "            yield {\n",
    "                \"input_ids\": inputs[\"input_ids\"].squeeze(0),\n",
    "                \"attention_mask\": inputs[\"attention_mask\"].squeeze(0),\n",
    "                \"labels\": targets[\"input_ids\"].squeeze(0)\n",
    "            }\n",
    "\n",
    "file_path = \"/Workspace/Users/lihi.kaspi@campus.technion.ac.il/training_data.parquet\"\n",
    "\n",
    "# Read the Parquet file into a Spark DataFrame\n",
    "df = spark.read.parquet(file_path)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Set the padding token\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "dataset = SparkSampledDataset(\n",
    "    spark_df=df,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=256,\n",
    "    sample_size=100,\n",
    "    seed=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ab10c0a9-7fb9-4759-a5f9-d7640c6a86d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f5ba5cf-d5b0-4634-8350-53a5f1ca5ca7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create the DataLoader\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, AdamW\n",
    "import torch\n",
    "dataloader = DataLoader(dataset, batch_size=4)\n",
    "#---- Model definition ----\n",
    "# Load GPT-2 model and tokenizer\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Define pad_token (GPT-2 doesn't have a native pad_token)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Move the model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# ---- Optimizer definition ----\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "EPOCHS = 10\n",
    "\n",
    "# ---- Training loop ----\n",
    "model.train()\n",
    "for epoch in range(EPOCHS):  # Number of epochs\n",
    "    i=0\n",
    "    for batch in dataloader:\n",
    "        print(batch['input_ids'].shape, i)\n",
    "        i+=1\n",
    "        optimizer.zero_grad()  # Reset gradients\n",
    "        outputs = model(**batch)  # Forward pass\n",
    "        loss = outputs.loss      # Compute loss\n",
    "        loss.backward()          # Backward pass\n",
    "        optimizer.step()         # Update model parameters\n",
    "\n",
    "    print(f\"Epoch {epoch + 1} completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "18e1d1c8-0731-45fc-a1e4-2d1192db2440",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d61c5bf7-a8f8-4166-a71c-901db205acd5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41d39484-ef36-413a-a69e-d314c87af332",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Save the model and tokenizer\n",
    "# model.save_pretrained(\"Workspace/Users/lihi.kaspi@campus.technion.ac.il/fine_tuned_gpt2\")\n",
    "# tokenizer.save_pretrained(\"Workspace/Users/lihi.kaspi@campus.technion.ac.il/fine_tuned_gpt2\")\n",
    "model.save_pretrained(\"/Workspace/Users/lihi.kaspi@campus.technion.ac.il/fine_tuned_gpt2\")\n",
    "tokenizer.save_pretrained(\"/Workspace/Users/lihi.kaspi@campus.technion.ac.il/fine_tuned_gpt2\")\n",
    "print(\"Model saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "be8db186-1ecf-40de-b167-fde7e06e8298",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import MultilayerPerceptronClassifier\n",
    "\n",
    "# Define the layers of the neural network\n",
    "# Input layer = number of features, hidden layers = user-defined, output layer = number of classes\n",
    "layers = [133, 64, 32, 3]\n",
    "\n",
    "# Initialize MLP Classifier\n",
    "mlp = MultilayerPerceptronClassifier(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"label\",\n",
    "    maxIter=100,\n",
    "    layers=layers,\n",
    "    blockSize=128,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "mlp_model = mlp.fit(train_multi_df)\n",
    "\n",
    "# Make predictions\n",
    "mlp_predictions = mlp_model.transform(test_multi_df)\n",
    "\n",
    "# Evaluate\n",
    "accuracy = evaluator.evaluate(mlp_predictions)\n",
    "print(f\"MLP Accuracy: {accuracy}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "GPT_fine_tuning",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
