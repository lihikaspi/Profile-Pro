{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "18bfc611-a9d3-4544-b843-6315fb84906a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Profile Pro: A LinkedIn Profile Optimizer\n",
    "## Final Project - Data Collection Lab (0940290)\n",
    "### Lihi Kaspi (214676140), Harel Oved (326042389) & Lior Zaphir (326482213)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99768cc0-a0cb-41a7-adaa-80ef43645fee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as f\n",
    "import pandas as pd\n",
    "from pyspark.ml.feature import CountVectorizer, Tokenizer, StringIndexer, VectorAssembler, Tokenizer, OneHotEncoder, Word2Vec, HashingTF, IndexToString\n",
    "from pyspark.ml.linalg import SparseVector, Vectors\n",
    "import numpy as np\n",
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "from pyspark.ml.classification import RandomForestClassifier, LogisticRegression\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.sql.window import Window\n",
    "from datetime import datetime\n",
    "import re\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b928cc60-2d1c-4e5f-b270-3d541c3a15c5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Relevant Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "22138499-831b-4f17-ae92-f17233177be1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# original datasets\n",
    "companies = spark.read.parquet('/dbfs/linkedin_train_data')\n",
    "profiles = spark.read.parquet('/dbfs/linkedin_people_train_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2757fd63-0941-4d9f-9ea6-846ff83b48b1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# new df with processed vector to go into the model\n",
    "processed_data = spark.read.parquet(\"/Workspace/Users/lihi.kaspi@campus.technion.ac.il/processed_data.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5a6ac684-25f5-447b-bc8c-1e42626c3268",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "processed_data = processed_data.withColumn(\n",
    "    'label', \n",
    "    f.when(f.col('profile_score') < 5, 0\n",
    "    ).when(f.col('profile_score') < 10, 1\n",
    "    ).when(f.col('profile_score') < 15, 2\n",
    "    ).when(f.col('profile_score') < 20, 3\n",
    "    ).otherwise(4)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5a7b10ab-4ba5-4252-8696-b66c82e85587",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8afcc7df-668f-421a-8237-17a41fc72644",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Good Profiles Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "71148ed7-d8e4-4a42-a657-626a6c78d63f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Training the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1c121c78-f44d-4036-b321-55952e8f15b1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### numeric models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99be653a-17ec-4776-a9d1-2e0161bdc57b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train_df, test_df = processed_data.randomSplit([0.7, 0.3], seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df5e663c-a8db-4d16-94ea-e4b0b42c929a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.functions import vector_to_array\n",
    "\n",
    "# Validate the training data\n",
    "train_df = train_df.na.drop()\n",
    "train_df = train_df.filter(f.size(vector_to_array(f.col('features'))) == 133)\n",
    "\n",
    "# Validate the test data\n",
    "test_df = test_df.na.drop()\n",
    "test_df = test_df.filter(f.size(vector_to_array(f.col('features'))) == 133)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9eac8963-ddb0-47c5-8958-799f6eecbe3f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "\n",
    "# Initialize the model\n",
    "rf = RandomForestRegressor(featuresCol=\"features\", labelCol=\"profile_score\")\n",
    "\n",
    "# Fit the model\n",
    "rf_model = rf.fit(train_df)\n",
    "\n",
    "# Make predictions\n",
    "rf_predictions = rf_model.transform(test_df)\n",
    "display(rf_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "26179a2d-b37c-4d5e-a10e-088fa7b7eaa9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "rf_predictions = rf_predictions.withColumn(\n",
    "    'accurate', \n",
    "    (f.col('profile_score') - 3 <= f.col('prediction')) & \n",
    "    (f.col('prediction') <= f.col('profile_score') + 3)\n",
    ")\n",
    "count_accurate = rf_predictions.where(f.col('accurate') == 1).count()\n",
    "len_df = rf_predictions.count()\n",
    "rf_accuracy = count_accurate / len_df\n",
    "print(rf_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "996a8d7b-d326-45dc-a69c-25c9ab4ee6f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import GBTRegressor\n",
    "\n",
    "# Initialize the model\n",
    "gbt = GBTRegressor(featuresCol=\"features\", labelCol=\"profile_score\")\n",
    "\n",
    "# Fit the model\n",
    "gbt_model = gbt.fit(train_df.select('features', 'profile_score'))\n",
    "\n",
    "# Make predictions\n",
    "gbt_predictions = gbt_model.transform(test_df)\n",
    "display(gbt_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c713a02a-b52d-45b2-9b3f-f57d5d295c59",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "gbt_predictions = gbt_predictions.withColumn(\n",
    "    'accurate', \n",
    "    (f.col('profile_score') - 5 <= f.col('prediction')) & \n",
    "    (f.col('prediction') <= f.col('profile_score') + 5)\n",
    ")\n",
    "# over_20 = gbt_predictions.where(f.col('profile_score') >= 30)\n",
    "count_accurate = gbt_predictions.where(f.col('accurate') == 1).count()\n",
    "len_df = gbt_predictions.count()\n",
    "gbt_accuracy = count_accurate / len_df\n",
    "print(gbt_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a0b0717b-a2f5-441f-8a07-6f9dd1779d15",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### multiclass classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a580db1-b485-4702-a48d-0cfe57ec5027",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9961ce5c-053b-4c5e-b26d-18f6710585f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# turn to multiclass\n",
    "multi_processed_data_3 = processed_data.withColumn(\n",
    "    'label', \n",
    "    f.when(f.col('profile_score') < 7, 0\n",
    "    ).when(f.col('profile_score') < 17, 1\n",
    "    ).otherwise(2)\n",
    ")\n",
    "train_multi_df_3, test_multi_df_3 = multi_processed_data_3.randomSplit([0.7, 0.3], seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "11585f10-0cc0-4cb8-ad77-c268c93da296",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.functions import vector_to_array\n",
    "\n",
    "# Validate the training data\n",
    "train_multi_df = train_multi_df.na.drop()\n",
    "train_multi_df = train_multi_df.filter(f.size(vector_to_array(f.col('features'))) == 133)\n",
    "\n",
    "# Validate the test data\n",
    "test_multi_df = test_multi_df.na.drop()\n",
    "test_multi_df = test_multi_df.filter(f.size(vector_to_array(f.col('features'))) == 133)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "65f80c91-a5c6-4c49-adac-1cd4013954ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import MultilayerPerceptronClassifier\n",
    "\n",
    "# Define the layers of the neural network\n",
    "# Input layer = number of features, hidden layers = user-defined, output layer = number of classes\n",
    "layers = [133, 64, 32, 3]\n",
    "\n",
    "# Initialize MLP Classifier\n",
    "mlp = MultilayerPerceptronClassifier(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"label\",\n",
    "    maxIter=100,\n",
    "    layers=layers,\n",
    "    blockSize=128,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "mlp_model = mlp.fit(train_multi_df)\n",
    "\n",
    "# Make predictions\n",
    "mlp_predictions = mlp_model.transform(test_multi_df)\n",
    "\n",
    "# Evaluate\n",
    "accuracy = evaluator.evaluate(mlp_predictions)\n",
    "print(f\"MLP Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5db4f989-5835-4e9e-bf92-7e0506d558c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# turn to multiclass\n",
    "multi_processed_data_4 = processed_data.withColumn(\n",
    "    'label', \n",
    "    f.when(f.col('profile_score') < 5, 0\n",
    "    ).when(f.col('profile_score') < 12, 1\n",
    "    ).when(f.col('profile_score') < 20, 2\n",
    "    ).otherwise(3)\n",
    ")\n",
    "train_multi_df_4, test_multi_df_4 = multi_processed_data_4.randomSplit([0.7, 0.3], seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9bc209ad-b16d-42a9-a961-9e48a5b1e9d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.functions import vector_to_array\n",
    "\n",
    "# Validate the training data\n",
    "train_multi_df_4 = train_multi_df_4.na.drop()\n",
    "train_multi_df_4 = train_multi_df_4.filter(f.size(vector_to_array(f.col('features'))) == 133)\n",
    "\n",
    "# Validate the test data\n",
    "test_multi_df_4 = test_multi_df_4.na.drop()\n",
    "test_multi_df_4 = test_multi_df_4.filter(f.size(vector_to_array(f.col('features'))) == 133)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b6518d6-1664-42dd-9a1a-c30882f8d854",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import MultilayerPerceptronClassifier\n",
    "\n",
    "# Define the layers of the neural network\n",
    "# Input layer = number of features, hidden layers = user-defined, output layer = number of classes\n",
    "layers = [133, 64, 32, 4]\n",
    "\n",
    "# Initialize MLP Classifier\n",
    "mlp = MultilayerPerceptronClassifier(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"label\",\n",
    "    maxIter=100,\n",
    "    layers=layers,\n",
    "    blockSize=128,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "mlp_model = mlp.fit(train_multi_df_4)\n",
    "\n",
    "# Make predictions\n",
    "mlp_predictions = mlp_model.transform(test_multi_df_4)\n",
    "\n",
    "# Evaluate\n",
    "accuracy = evaluator.evaluate(mlp_predictions)\n",
    "print(f\"MLP Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "14646248-e0bf-40cf-a524-1608d8428c5f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# turn to multiclass\n",
    "multi_processed_data_5 = processed_data.withColumn(\n",
    "    'label', \n",
    "    f.when(f.col('profile_score') < 5, 0\n",
    "    ).when(f.col('profile_score') < 10, 1\n",
    "    ).when(f.col('profile_score') < 15, 2\n",
    "    ).when(f.col('profile_score') < 20, 3\n",
    "    ).otherwise(4)\n",
    ")\n",
    "train_multi_df_5, test_multi_df_5 = multi_processed_data_5.randomSplit([0.7, 0.3], seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "17ac8cf9-ab34-4112-bdf4-878cba1994ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.functions import vector_to_array\n",
    "\n",
    "# Validate the training data\n",
    "train_multi_df_5 = train_multi_df_5.na.drop()\n",
    "train_multi_df_5 = train_multi_df_5.filter(f.size(vector_to_array(f.col('features'))) == 133)\n",
    "\n",
    "# Validate the test data\n",
    "test_multi_df_5 = test_multi_df_5.na.drop()\n",
    "test_multi_df_5 = test_multi_df_5.filter(f.size(vector_to_array(f.col('features'))) == 133)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "514d7b31-d38a-4d9b-8a2e-2a568c34ac69",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import MultilayerPerceptronClassifier\n",
    "\n",
    "# Define the layers of the neural network\n",
    "# Input layer = number of features, hidden layers = user-defined, output layer = number of classes\n",
    "layers = [133, 64, 32, 5]\n",
    "\n",
    "# Initialize MLP Classifier\n",
    "mlp = MultilayerPerceptronClassifier(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"label\",\n",
    "    maxIter=100,\n",
    "    layers=layers,\n",
    "    blockSize=128,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "mlp_model = mlp.fit(train_multi_df_5)\n",
    "\n",
    "# Make predictions\n",
    "mlp_predictions = mlp_model.transform(test_multi_df_5)\n",
    "\n",
    "# Evaluate\n",
    "accuracy = evaluator.evaluate(mlp_predictions)\n",
    "print(f\"MLP Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1352ef53-aa69-48b1-a7a6-d27590481674",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sample = mlp_predictions.select('prediction').toPandas()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(sample['prediction'], bins=30, edgecolor='k', alpha=0.7)\n",
    "plt.title('Histogram of Profile Scores')\n",
    "plt.xlabel('Profile Score')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c20c6171-2c93-4759-a167-ac92d564d0ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "when checking accuracy - accepted score should be between (real_score-5, real_score+5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ef28b933-4f71-4731-a32f-6bf219267eaf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Profile Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4ab612f1-eee5-42ea-baed-78284e6c9683",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 'about' Section Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a83e58b4-215f-40fe-a79a-0989590d452e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# take: about (if not null), position, job title, reccomendations \n",
    "# --> return: a sentence or two describing the person and job (in a new column called 'new_about')\n",
    "# if all null: return message 'could not generate a short bio -- add more information to your profile' (put null in 'new_about' and add message in a new column called 'about_message')\n",
    "\n",
    "good_profiles_df = ''\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4cab94df-d4dd-490d-bb02-680cf93d4f81",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def generate_about(input_prompt, model, tokenizer):\n",
    "    # Tokenize the input prompt\n",
    "    inputs = tokenizer(input_prompt, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "\n",
    "    # Generate output text\n",
    "    outputs = model.generate(inputs[\"input_ids\"], max_length=150, num_beams=5, early_stopping=True)\n",
    "\n",
    "    # Decode and return the generated text\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Example usage\n",
    "input_prompt = \"City: New York, Education: Master's in Data Science from Columbia University, Name: Jane Doe, Position: Data Scientist\"\n",
    "about_section = generate_about(input_prompt, model, tokenizer)\n",
    "print(\"Generated About Section:\", about_section)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b0db48a2-457c-45be-9c59-118647dca859",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Improvements and Suggetions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9bf77270-eb22-40f3-b9ea-1af23ad698a7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "score ranking:\n",
    "- excellent score - 90+ and no suggestions\n",
    "- high score - 90+ and atleast one suggestion\n",
    "- medium high score - 60-90\n",
    "- medium score - 40-60\n",
    "- medium low score - 20-40\n",
    "- low score - 20>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "187a0312-7fa5-4b34-9bd9-af75a6f1d18c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "score_messages = {\n",
    "    'excellent score': 'Your profile is excellent, keep it up!',\n",
    "    'high score': 'Your profile is very strong, Check the suggestions to make it excellent',\n",
    "    'medium high score': 'Your profile is good, Try to follow the suggestions to make it even better',\n",
    "    'medium score': 'Your profile could use a few improvements, Try to follow the suggestions to make it even better',\n",
    "    'medium low score': 'Your profile needs to improve, Try to follow the suggestion to make it better',\n",
    "    'low score': 'Your profile is weak, Try to follow the suggestion to make it better',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "15d4234a-8ae9-4b09-8fc8-a9eafb9a5681",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "missing_field_messages = {\n",
    "    'no_experience': 'Add previous/current comapnies you worked in', \n",
    "    'no_education': 'List your degrees and schools you graduated from',\n",
    "    'no_about': 'Add a short bio about yourself, here is a suggestion: ',\n",
    "    'suggested_about': 'Try out this about section: ',\n",
    "    'no_company': 'Add the company you currently work in',\n",
    "    'no_languages': 'List all the languages you know and the level of knowledge',\n",
    "    'no_position': 'Add the position you are currently in',\n",
    "    'no_posts': 'Tell your friends about projects you currently work on',\n",
    "    'no_recommendations': 'Ask a colleague to write a few words about you',\n",
    "    'missing_experience': 'There is a gap in your resume, Don\\'t forget to add all of the previous comapnies you worked in',\n",
    "    'low_followers': 'Ask your colleagues and friends to follow you on LinkedIn!'\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5321e47c-06aa-45f3-a950-fb0521d824e8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# placeholder name for the predictions: predicted_df (has all the previous columns + score predictions)\n",
    "\n",
    "predicted_df = predicted_df.withColumn(\n",
    "  'score_rank', \n",
    "  f.when(f.col('score') < 20, 'low score'\n",
    "  ).when(f.col('score') < 40, 'medium low score'\n",
    "  ).when(f.col('score') < 60, 'medium score'\n",
    "  ).when(f.col('score') < 90, 'medium high score'\n",
    "  ).when(f.col('filled_percent') < 100, 'high score'\n",
    "  ).otherwise('excellent score')\n",
    ")\n",
    "\n",
    "predicted_df = predicted_df.withColumn(\n",
    "  'score_message',\n",
    "  score_messages.get(f.col('score_rank'))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "82696174-9da2-4377-9508-61ae90a1ce13",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# find if there are gaps in the experience array (name new column: 'gap_in_experience')\n",
    "# TODO: Binary or explicit time period? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "70f31f75-c24a-4033-83d0-67280a4a84f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "predicted_df = predicted_df.withColumn('suggestions', f.array())\n",
    "\n",
    "predicted_df = predicted_df.withColumn(\n",
    "  'suggestions',\n",
    "  f.array(\n",
    "    f.when(\n",
    "      f.size(f.col('education')) == 0, \n",
    "      missing_field_messages.get('no_education')),\n",
    "    f.when(\n",
    "      f.size(f.col('current_company')) == 0, \n",
    "      missing_field_messages.get('no_company')),\n",
    "    f.when(\n",
    "      f.size(f.col('languages')) == 0, \n",
    "      missing_field_messages.get('no_languages')),\n",
    "    f.when(\n",
    "      f.size(f.col('posts')) == 0, \n",
    "      missing_field_messages.get('no_posts')),\n",
    "    f.when(\n",
    "      f.col('recommendations_count') == 0, \n",
    "      missing_field_messages.get('no_recommendations')),\n",
    "    f.when(\n",
    "      f.col('about').isNull() & f.col('new_about').isNotNull(), \n",
    "      missing_field_messages.get('no_about') + f.col('new_about')),\n",
    "    f.when(\n",
    "      f.col('about').isNotNull() & f.col('new_about').isNotNull() & f.col('score') < 90, \n",
    "      missing_field_messages.get('suggested_about') + f.col('new_about')),\n",
    "    f.when(\n",
    "      f.col('about_message').isNotNull(), \n",
    "      f.col('about_message')),\n",
    "    f.when(\n",
    "      f.col('position').isNull(),\n",
    "      missing_field_messages.get('no_position')),\n",
    "    f.when(\n",
    "      f.col('followers') < 20,\n",
    "      missing_field_messages.get('low_followers')),\n",
    "    f.when(\n",
    "      f.size(f.col('experience')) == 0, \n",
    "      missing_field_messages.get('no_experience')), \n",
    "    f.when(\n",
    "      f.col('gap_in_experience').isNotNull(), # TODO: adapt to binary or time period\n",
    "      missing_field_messages.get('missing_experience'))\n",
    "  )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "31fb62a8-cb9f-4367-921c-2827fa84c4fe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "optemized_df = predicted_df.select('name', 'id', 'url', 'score_rank', 'score_message', 'suggestions')\n",
    "display(optemized_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0603f804-a20e-46cb-8993-170f2a7e101c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Re-Evaluating the Profiles Post Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "540d27c4-8b96-460f-9b44-eeeddd8baed0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# need to see how listening to suggestions improve score - present the result by increasing number of sugggestions"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Profile_Pro",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
