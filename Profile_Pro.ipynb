{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "18bfc611-a9d3-4544-b843-6315fb84906a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Profile Pro: A LinkedIn Profile Optimizer\n",
    "## Final Project - Data Collection Lab (0940290)\n",
    "### Lihi Kaspi (214676140), Harel Oved (326042389) & Lior Zaphir (326482213)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99768cc0-a0cb-41a7-adaa-80ef43645fee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as f\n",
    "import pandas as pd\n",
    "from pyspark.ml.feature import CountVectorizer, Tokenizer, StringIndexer, VectorAssembler, Tokenizer, OneHotEncoder, Word2Vec, HashingTF, IndexToString\n",
    "from pyspark.ml.linalg import SparseVector, Vectors\n",
    "import numpy as np\n",
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "from pyspark.ml.classification import RandomForestClassifier, LogisticRegression\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.sql.window import Window\n",
    "from datetime import datetime\n",
    "import re\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b928cc60-2d1c-4e5f-b270-3d541c3a15c5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Relevant Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "22138499-831b-4f17-ae92-f17233177be1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# original datasets\n",
    "companies = spark.read.parquet('/dbfs/linkedin_train_data')\n",
    "profiles = spark.read.parquet('/dbfs/linkedin_people_train_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "448a79e0-271c-4f98-a8f1-101a1fabf782",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "profiles.select(['city', 'education', 'name', 'position', 'about']).dropna().display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2757fd63-0941-4d9f-9ea6-846ff83b48b1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# new df with processed vector to go into the model\n",
    "processed_data = spark.read.parquet(\"/Workspace/Users/lihi.kaspi@campus.technion.ac.il/processed_data.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "594150a3-c3ad-4fa2-946f-580eb2fde067",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Pre process good profile data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf65245f-dbec-41cf-9465-07a2eee3b494",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, concat_ws, udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "good_profiles_df = profiles.select(['city', 'education', 'name', 'position', 'about']).dropna()\n",
    "\n",
    "\n",
    "\n",
    "def strip_and_choose_first(str_lst):\n",
    "    return str_lst.strip(\"[]\").split(\", \")[0]\n",
    "\n",
    "\n",
    "# UDF to process the 'education' field (extract degree and school information)\n",
    "def process_education(degree, field, title):\n",
    "    # Extract degree, field, and school title from each education entry\n",
    "    degree = strip_and_choose_first(degree)\n",
    "    field = strip_and_choose_first(field)\n",
    "    title = strip_and_choose_first(title)\n",
    "    edu_details = f\"{degree} in {field} from {title}\"\n",
    "    return edu_details\n",
    "\n",
    "# Register UDF\n",
    "process_education_udf = udf(process_education, StringType())\n",
    "\n",
    "# Filter rows where the education column is not empty\n",
    "filtered_df = good_profiles_df.filter((col(\"education\").isNotNull()) & (col(\"education\") != f.lit([])))\n",
    "\n",
    "filtered_df = filtered_df.withColumn('degree', col('education').getField('degree').cast('string'))\n",
    "filtered_df = filtered_df.withColumn('field', col('education').getField('field').cast('string'))\n",
    "filtered_df = filtered_df.withColumn('school', col('education').getField('title').cast('string'))\n",
    "\n",
    "# Process the DataFrame\n",
    "good_profiles_df = filtered_df.withColumn(\"processed_education\", \n",
    "                                          process_education_udf(col('degree'), col('field'), col('school')))\n",
    "\n",
    "# Show the resulting DataFrame\n",
    "processed_df = good_profiles_df.withColumn(\n",
    "                                    \"input_prompt\",\n",
    "                                    concat_ws(\n",
    "                                        \", \",\n",
    "                                        col(\"city\"),\n",
    "                                        col(\"processed_education\"),\n",
    "                                        col(\"name\"),\n",
    "                                        col(\"position\"),\n",
    "                                    )\n",
    "                            )\n",
    "processed_df.display()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b56ab24a-e401-418f-8572-5c256f69cb38",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c82b24a7-d4c5-43a3-a671-1bf898acc566",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Tokenization and preparation of input column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d61b7eac-da50-4863-ae75-2ec82b51af49",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sparknlp.base import DocumentAssembler\n",
    "from sparknlp.annotator import Tokenizer\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Document assembler\n",
    "document_assembler = DocumentAssembler().setInputCol(\"input_prompt\").setOutputCol(\"document\")\n",
    "\n",
    "# Tokenizer\n",
    "tokenizer = Tokenizer().setInputCols([\"document\"]).setOutputCol(\"token\")\n",
    "\n",
    "# Define the pipeline\n",
    "nlp_pipeline = Pipeline(stages=[document_assembler, tokenizer])\n",
    "\n",
    "# Apply the pipeline to the DataFrame\n",
    "tokenized_df = nlp_pipeline.fit(processed_df).transform(processed_df)\n",
    "\n",
    "# Show tokenized data\n",
    "tokenized_df.select(\"input_prompt\", \"token\").display()\n",
    "\n",
    "# Save with overwrite mode\n",
    "tokenized_df.select(\"input_prompt\", \"about\", \"token\").write.mode(\"overwrite\").parquet(\"/Workspace/Users/lihi.kaspi@campus.technion.ac.il/training_data.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba462500-e111-43c3-a937-a1cb80eeea02",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "file_path = \"/Workspace/Users/lihi.kaspi@campus.technion.ac.il/training_data.parquet\"\n",
    "\n",
    "# Read the Parquet file into a Spark DataFrame\n",
    "df = spark.read.parquet(file_path)\n",
    "\n",
    "# Show the DataFrame schema and data\n",
    "df.printSchema()\n",
    "\n",
    "df.write.mode(\"overwrite\").json(\"/Workspace/Users/lihi.kaspi@campus.technion.ac.il/tokenized_data.json\")\n",
    "df = spark.read.json(\"/Workspace/Users/lihi.kaspi@campus.technion.ac.il/tokenized_data.json\")\n",
    "df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cff9a63b-9a66-4bd4-b21c-2e6533e1c302",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import json\n",
    "\n",
    "# Custom PyTorch dataset\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, json_file, tokenizer, max_length=512):\n",
    "        # Load JSON data\n",
    "        with open(json_file, \"r\") as f:\n",
    "            self.data = [json.loads(line) for line in f]\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Extract tokenized input and target\n",
    "        item = self.data[idx]\n",
    "        input_tokens = \" \".join(item[\"result\"])  # Join tokenized input\n",
    "        target_text = item[\"about\"]             # Target text\n",
    "\n",
    "        # Tokenize inputs and targets\n",
    "        inputs = self.tokenizer(input_tokens, truncation=True, padding=\"max_length\", max_length=self.max_length, return_tensors=\"pt\")\n",
    "        targets = self.tokenizer(target_text, truncation=True, padding=\"max_length\", max_length=self.max_length, return_tensors=\"pt\")\n",
    "\n",
    "        # Add labels\n",
    "        inputs[\"labels\"] = targets[\"input_ids\"]\n",
    "        return {key: torch.squeeze(val) for key, val in inputs.items()}\n",
    "\n",
    "# Load GPT-2 tokenizer and model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Initialize dataset and dataloader\n",
    "dataset = CustomDataset(\"/dbfs/Workspace/Users/lihi.kaspi@campus.technion.ac.il/tokenized_data.json\", tokenizer)\n",
    "dataloader = DataLoader(dataset, batch_size=8, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0e67798a-3aa1-42cf-be6c-89f33001ea5b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a31eeccc-a69d-438b-8fcf-cf972390ba5f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Set up optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# Training loop\n",
    "model.train()\n",
    "for epoch in range(3):  # Number of epochs\n",
    "    for batch in dataloader:\n",
    "        optimizer.zero_grad()  # Reset gradients\n",
    "        outputs = model(**batch)  # Forward pass\n",
    "        loss = outputs.loss      # Compute loss\n",
    "        loss.backward()          # Backward pass\n",
    "        optimizer.step()         # Update model parameters\n",
    "\n",
    "    print(f\"Epoch {epoch + 1} completed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bd057c7d-ba26-4f5c-ae24-0c2573ca2957",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "11b3e928-b2f6-495b-b3bd-fa9e7b04f21b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Save the model and tokenizer\n",
    "model.save_pretrained(\"path/to/fine_tuned_gpt2\")\n",
    "tokenizer.save_pretrained(\"path/to/fine_tuned_gpt2\")\n",
    "print(\"Model saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "58886e83-c607-4f54-b522-9dcda122c099",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "13edb587-0aef-4748-8554-0bd43374f03f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def generate_about(input_prompt, model, tokenizer):\n",
    "    # Tokenize the input prompt\n",
    "    inputs = tokenizer(input_prompt, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "\n",
    "    # Generate output text\n",
    "    outputs = model.generate(inputs[\"input_ids\"], max_length=150, num_beams=5, early_stopping=True)\n",
    "\n",
    "    # Decode and return the generated text\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Example usage\n",
    "input_prompt = \"City: New York, Education: Master's in Data Science from Columbia University, Name: Jane Doe, Position: Data Scientist\"\n",
    "about_section = generate_about(input_prompt, model, tokenizer)\n",
    "print(\"Generated About Section:\", about_section)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5a7b10ab-4ba5-4252-8696-b66c82e85587",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8afcc7df-668f-421a-8237-17a41fc72644",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Good Profiles Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2bc984ba-29bf-4fa5-b706-5f718538d315",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### i want to predit a numeric score and not binary label -- will be better for the final stage of suggesting improvemnts\n",
    "### maybe predict categories of score (example below)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "71148ed7-d8e4-4a42-a657-626a6c78d63f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Training the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "64dc7421-9343-4d12-a997-5913ff8f36a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "possible models:\n",
    "- Decision Tree Regressor\n",
    "- Random Forest Regressor\n",
    "- Gradient-Boosted Trees Regressor\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1c121c78-f44d-4036-b321-55952e8f15b1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### numeric models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99be653a-17ec-4776-a9d1-2e0161bdc57b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train_df, test_df = processed_data.randomSplit([0.7, 0.3], seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df5e663c-a8db-4d16-94ea-e4b0b42c929a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Validate the training data\n",
    "train_df = train_df.na.drop()\n",
    "train_df = train_df.filter(col(\"features\").isNotNull() & col(\"profile_score\").isNotNull())\n",
    "\n",
    "# Validate the test data\n",
    "test_df = test_df.na.drop()\n",
    "test_df = test_df.filter(col(\"features\").isNotNull() & col(\"profile_score\").isNotNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9eac8963-ddb0-47c5-8958-799f6eecbe3f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "\n",
    "# Filter out rows with inconsistent feature vector lengths\n",
    "train_df_filtered = train_df.dropna(subset=['features', 'profile_score']) \\\n",
    "                            .filter(f.size(f.col('features')) == 133)\n",
    "\n",
    "# Initialize the model\n",
    "rf = RandomForestRegressor(featuresCol=\"features\", labelCol=\"profile_score\")\n",
    "\n",
    "# Fit the model\n",
    "rf_model = rf.fit(train_df.dropna(subset=['features', 'profile_score']))\n",
    "\n",
    "# Make predictions\n",
    "rf_predictions = rf_model.transform(test_df)\n",
    "display(rf_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "26179a2d-b37c-4d5e-a10e-088fa7b7eaa9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "rf_predictions = rf_predictions.withColumn('accurate', f.expr(\"cast(profile_score - 5 <= prediction <= profile_score + 5) as int\"))\n",
    "count_accurate = rf_predictions.where(f.col('accurate') == 1).count()\n",
    "len_df = rf_predictions.count()\n",
    "rf_accuracy = count_accurate / len_df\n",
    "print(rf_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "996a8d7b-d326-45dc-a69c-25c9ab4ee6f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import GBTRegressor\n",
    "\n",
    "# Initialize the model\n",
    "gbt = GBTRegressor(featuresCol=\"features\", labelCol=\"profile_score\")\n",
    "\n",
    "# Fit the model\n",
    "gbt_model = gbt.fit(train_df)\n",
    "\n",
    "# Make predictions\n",
    "gbt_predictions = gbt_model.transform(test_df)\n",
    "display(gbt_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c713a02a-b52d-45b2-9b3f-f57d5d295c59",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "gbt_predictions = gbt_predictions.withColumn('accurate', f.expr(\"cast(profile_score - 5 <= prediction <= profile_score + 5) as int\")) \n",
    "count_accurate = gbt_predictions.where(f.col('accurate') == 1).count()\n",
    "len_df = gbt_predictions.count()\n",
    "gbt_accuracy = count_accurate / len_df\n",
    "print(gbt_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a0b0717b-a2f5-441f-8a07-6f9dd1779d15",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### multiclass classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9961ce5c-053b-4c5e-b26d-18f6710585f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "score_labels = []\n",
    "# turn to multiclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7a580db1-b485-4702-a48d-0cfe57ec5027",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"profile_score\", predictionCol=\"prediction\", metricName=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "65f80c91-a5c6-4c49-adac-1cd4013954ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import MultilayerPerceptronClassifier\n",
    "\n",
    "# Define the layers of the neural network\n",
    "# Input layer = number of features, hidden layers = user-defined, output layer = number of classes\n",
    "layers = [133, 64, 32, 5]\n",
    "\n",
    "# Initialize MLP Classifier\n",
    "mlp = MultilayerPerceptronClassifier(featuresCol=\"features\", labelCol=\"score_labels\", maxIter=100, layers=layers, blockSize=128, seed=1234)\n",
    "\n",
    "# Train the model\n",
    "mlp_model = mlp.fit(training_data)\n",
    "\n",
    "# Make predictions\n",
    "mlp_predictions = mlp_model.transform(test_data)\n",
    "\n",
    "# Evaluate\n",
    "accuracy = evaluator.evaluate(mlp_predictions)\n",
    "print(f\"MLP Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3100db3c-4d73-4fab-bb62-e8f899d6d172",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e622f6df-80aa-47d0-a5fa-7518ffd81013",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c20c6171-2c93-4759-a167-ac92d564d0ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "when checking accuracy - accepted score should be between (real_score-5, real_score+5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ef28b933-4f71-4731-a32f-6bf219267eaf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Profile Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4ab612f1-eee5-42ea-baed-78284e6c9683",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 'about' Section Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a83e58b4-215f-40fe-a79a-0989590d452e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# take: about (if not null), position, job title, reccomendations \n",
    "# --> return: a sentence or two describing the person and job (in a new column called 'new_about')\n",
    "# if all null: return message 'could not generate a short bio -- add more information to your profile' (put null in 'new_about' and add message in a new column called 'about_message')\n",
    "\n",
    "good_profiles_df = ''\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b0db48a2-457c-45be-9c59-118647dca859",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Improvements and Suggetions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9bf77270-eb22-40f3-b9ea-1af23ad698a7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "score ranking:\n",
    "- excellent score - 90+ and no suggestions\n",
    "- high score - 90+ and atleast one suggestion\n",
    "- medium high score - 60-90\n",
    "- medium score - 40-60\n",
    "- medium low score - 20-40\n",
    "- low score - 20>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "187a0312-7fa5-4b34-9bd9-af75a6f1d18c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "score_messages = {\n",
    "    'excellent score': 'Your profile is excellent, keep it up!',\n",
    "    'high score': 'Your profile is very strong, Check the suggestions to make it excellent',\n",
    "    'medium high score': 'Your profile is good, Try to follow the suggestions to make it even better',\n",
    "    'medium score': 'Your profile could use a few improvements, Try to follow the suggestions to make it even better',\n",
    "    'medium low score': 'Your profile needs to improve, Try to follow the suggestion to make it better',\n",
    "    'low score': 'Your profile is weak, Try to follow the suggestion to make it better',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "15d4234a-8ae9-4b09-8fc8-a9eafb9a5681",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "missing_field_messages = {\n",
    "    'no_experience': 'Add previous/current comapnies you worked in', \n",
    "    'no_education': 'List your degrees and schools you graduated from',\n",
    "    'no_about': 'Add a short bio about yourself, here is a suggestion: ',\n",
    "    'suggested_about': 'Try out this about section: ',\n",
    "    'no_company': 'Add the company you currently work in',\n",
    "    'no_languages': 'List all the languages you know and the level of knowledge',\n",
    "    'no_position': 'Add the position you are currently in',\n",
    "    'no_posts': 'Try to be more active with you account',\n",
    "    'no_recommendations': 'Ask a colleague to write a few words about you',\n",
    "    'missing_experience': 'There is a gap in your resume, Don\\'t forget to add all of the previous comapnies you worked in',\n",
    "    'low_followers': 'Ask your colleagues and friends to follow you on LinkedIn!'\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5321e47c-06aa-45f3-a950-fb0521d824e8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# placeholder name for the predictions: predicted_df (has all the previous columns + score predictions)\n",
    "\n",
    "predicted_df = predicted_df.withColumn(\n",
    "  'score_rank', \n",
    "  f.when(f.col('score') < 20, 'low score'\n",
    "  ).when(f.col('score') < 40, 'medium low score'\n",
    "  ).when(f.col('score') < 60, 'medium score'\n",
    "  ).when(f.col('score') < 90, 'medium high score'\n",
    "  ).when(f.col('filled_percent') < 100, 'high score'\n",
    "  ).otherwise('excellent score')\n",
    ")\n",
    "\n",
    "predicted_df = predicted_df.withColumn(\n",
    "  'score_message',\n",
    "  score_messages.get(f.col('score_rank'))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "82696174-9da2-4377-9508-61ae90a1ce13",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# find if there are gaps in the experience array (name new column: 'gap_in_experience')\n",
    "# TODO: Binary or explicit time period? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "70f31f75-c24a-4033-83d0-67280a4a84f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "predicted_df = predicted_df.withColumn('suggestions', f.array())\n",
    "\n",
    "predicted_df = predicted_df.withColumn(\n",
    "  'suggestions',\n",
    "  f.array(\n",
    "    f.when(\n",
    "      f.size(f.col('education')) == 0, \n",
    "      missing_field_messages.get('no_education')),\n",
    "    f.when(\n",
    "      f.size(f.col('current_company')) == 0, \n",
    "      missing_field_messages.get('no_company')),\n",
    "    f.when(\n",
    "      f.size(f.col('languages')) == 0, \n",
    "      missing_field_messages.get('no_languages')),\n",
    "    f.when(\n",
    "      f.size(f.col('posts')) == 0, \n",
    "      missing_field_messages.get('no_posts')),\n",
    "    f.when(\n",
    "      f.col('recommendations_count') == 0, \n",
    "      missing_field_messages.get('no_recommendations')),\n",
    "    f.when(\n",
    "      f.col('about').isNull() & f.col('new_about').isNotNull(), \n",
    "      missing_field_messages.get('no_about') + f.col('new_about')),\n",
    "    f.when(\n",
    "      f.col('about').isNotNull() & f.col('new_about').isNotNull() & f.col('score') < 90, \n",
    "      missing_field_messages.get('suggested_about') + f.col('new_about')),\n",
    "    f.when(\n",
    "      f.col('about_message').isNotNull(), \n",
    "      f.col('about_message')),\n",
    "    f.when(\n",
    "      f.col('position').isNull(),\n",
    "      missing_field_messages.get('no_position')),\n",
    "    f.when(\n",
    "      f.col('followers') < 20,\n",
    "      missing_field_messages.get('low_followers')),\n",
    "    f.when(\n",
    "      f.size(f.col('experience')) == 0, \n",
    "      missing_field_messages.get('no_experience')), \n",
    "    f.when(\n",
    "      f.col('gap_in_experience').isNotNull(), # TODO: adapt to binary or time period\n",
    "      missing_field_messages.get('missing_experience'))\n",
    "  )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "31fb62a8-cb9f-4367-921c-2827fa84c4fe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "optemized_df = predicted_df.select('name', 'id', 'url', 'score_rank', 'score_message', 'suggestions')\n",
    "display(optemized_df)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Profile_Pro",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
