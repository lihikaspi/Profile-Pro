{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "18bfc611-a9d3-4544-b843-6315fb84906a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Profile Pro: A LinkedIn Profile Optimizer\n",
    "## Final Project - Data Collection Lab (0940290)\n",
    "### Lihi Kaspi (214676140), Harel Oved (326042389) & Lior Zaphir (326482213)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99768cc0-a0cb-41a7-adaa-80ef43645fee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as f\n",
    "import pandas as pd\n",
    "from pyspark.ml.feature import CountVectorizer, Tokenizer, StringIndexer, VectorAssembler, Tokenizer, OneHotEncoder, Word2Vec, HashingTF, IndexToString\n",
    "from pyspark.ml.linalg import SparseVector, Vectors\n",
    "import numpy as np\n",
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "from pyspark.ml.classification import RandomForestClassifier, LogisticRegression\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.sql.window import Window\n",
    "from datetime import datetime\n",
    "import re\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b928cc60-2d1c-4e5f-b270-3d541c3a15c5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Relevant Data and Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ecf75da4-9ddb-47ca-80f5-b7a9a26e218d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## i'm moving all the code cells that create a parquet file to different notebooks so we don't have to skip cells when running this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "22138499-831b-4f17-ae92-f17233177be1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# original datasets\n",
    "companies = spark.read.parquet('/dbfs/linkedin_train_data')\n",
    "profiles = spark.read.parquet('/dbfs/linkedin_people_train_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f4ef8341-14db-4057-a4e7-e223adc0b954",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# new df of profiled with their \"good profile\" score -- code can be found in \"Profile Score Calculation\"\n",
    "profiles_with_scores = spark.read.parquet(\"profiles_with_scores.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba01fdc3-1a17-4831-ae1c-ff781e1a8ed9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Data Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fcfb979c-f228-4b14-9fc6-be5c23545a21",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# when you're done move the imports to the top !!!!!!!!!!!! OK!!!!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa8da804-902c-4357-8afc-70c13337ef97",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import CountVectorizer, Tokenizer, StringIndexer, VectorAssembler, Tokenizer, OneHotEncoder, Word2Vec, HashingTF, IndexToString\n",
    "from pyspark.ml.linalg import SparseVector, Vectors\n",
    "import numpy as np\n",
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "from pyspark.ml.classification import RandomForestClassifier, LogisticRegression\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from sparknlp.base import DocumentAssembler\n",
    "from sparknlp.annotator import Tokenizer, StopWordsCleaner, WordEmbeddingsModel, SentenceEmbeddings\n",
    "from pyspark.ml.feature import VectorAssembler, OneHotEncoder, StringIndexer\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "from pyspark.sql.types import ArrayType, FloatType\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.ml.linalg import VectorUDT, DenseVector\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "import pyspark.sql.functions as f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6c130059-786d-4e8a-8687-5f9c001e54f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train_df, test_df = profiles_df.randomSplit([0.8, 0.2], seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1da28f22-8baf-4bbd-909f-632e01257a8c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sparknlp.pretrained import ResourceDownloader\n",
    "print(ResourceDownloader.showPublicModels(\"word_embedding\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13c8a2b1-279d-421d-a576-a922937a5b43",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import sparknlp\n",
    "print(sparknlp.version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b2b99d8-6129-4b43-bdf2-f41f938b924a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1. Preprocess `about` using Spark NLP\n",
    "document_assembler = DocumentAssembler() \\\n",
    "    .setInputCol(\"about\") \\\n",
    "    .setOutputCol(\"document\")\n",
    "\n",
    "tokenizer = Tokenizer() \\\n",
    "    .setInputCols([\"document\"]) \\\n",
    "    .setOutputCol(\"token\")\n",
    "\n",
    "stopwords_cleaner = StopWordsCleaner() \\\n",
    "    .setInputCols([\"token\"]) \\\n",
    "    .setOutputCol(\"clean_tokens\")\n",
    "\n",
    "embeddings = WordEmbeddingsModel.pretrained() \\\n",
    "    .setInputCols([\"document\", \"clean_tokens\"]) \\\n",
    "    .setOutputCol(\"embeddings\")\n",
    "\n",
    "sentence_embeddings = SentenceEmbeddings() \\\n",
    "    .setInputCols([\"document\", \"embeddings\"]) \\\n",
    "    .setOutputCol(\"about_embeddings\")\n",
    "\n",
    "nlp_pipeline = Pipeline(stages=[document_assembler, tokenizer, stopwords_cleaner, embeddings, sentence_embeddings])\n",
    "\n",
    "# Apply NLP Pipeline\n",
    "nlp_model = nlp_pipeline.fit(train_df)\n",
    "processed_data = nlp_model.transform(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f6626ce-6fcf-4d43-bcc3-2cd176fd318a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 2. Numerical Features\n",
    "processed_data = processed_data.withColumn(\"num_education\", f.size(f.col(\"education\"))) \\\n",
    "    .withColumn(\"num_experience\", f.size(f.col(\"experience\"))) \\\n",
    "    .withColumn(\"num_languages\", f.size(f.col(\"languages\"))) \\\n",
    "    .withColumn(\"total_followers\", f.col(\"followers\")) \\\n",
    "    .withColumn(\"recommendations\", f.col(\"recommendations_count\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf99bcd5-236c-4fb5-a5a9-afbfee757c17",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 3. Categorical Features - TODO USE THEM???\n",
    "# Index and encode categorical columns\n",
    "# indexer = StringIndexer(inputCol=\"country_code\", outputCol=\"country_code_index\")\n",
    "# encoder = OneHotEncoder(inputCol=\"country_code_index\", outputCol=\"country_code_vec\")\n",
    "# cat_pipeline = Pipeline(stages=[indexer, encoder])\n",
    "\n",
    "# Fit and transform categorical pipeline\n",
    "# cat_model = cat_pipeline.fit(processed_data)\n",
    "# processed_data = cat_model.transform(processed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "329a0c17-366d-44da-8dad-a6770f26c60f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 4. Assemble features\n",
    "# assembler = VectorAssembler(inputCols=[\n",
    "#     \"about_embeddings\", \"num_education\", \"num_experience\", \"num_languages\",\n",
    "#     \"total_followers\", \"recommendations\", \"country_code_vec\"\n",
    "# ], outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a6ef2f1c-d2b7-4511-ad8a-64f9affd60eb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# from pyspark.sql.functions import udf\n",
    "# from pyspark.ml.linalg import VectorUDT, DenseVector\n",
    "# from pyspark.ml.feature import VectorAssembler\n",
    "# import pyspark.sql.functions as f\n",
    "\n",
    "# # UDF to extract embeddings and convert to DenseVector\n",
    "# def extract_embeddings(embeddings):\n",
    "#     return DenseVector(embeddings[0].embeddings) if embeddings else DenseVector([])\n",
    "\n",
    "# extract_embeddings_udf = udf(extract_embeddings, VectorUDT())\n",
    "\n",
    "# # Apply the UDF to extract embeddings\n",
    "# processed_data = processed_data.withColumn(\"about_embeddings_vector\", extract_embeddings_udf(f.col(\"about_embeddings\")))\n",
    "\n",
    "# # Assemble features\n",
    "# assembler = VectorAssembler(\n",
    "#     inputCols=[\n",
    "#         \"about_embeddings_vector\", \"num_education\", \"num_experience\", \"num_languages\",\n",
    "#         \"total_followers\", \"recommendations\"\n",
    "#     ],\n",
    "#     outputCol=\"features\"\n",
    "# )\n",
    "\n",
    "# final_data = assembler.transform(processed_data)\n",
    "\n",
    "# # Select relevant columns\n",
    "# final_data = final_data.select(\"features\", \"filled_precent\")\n",
    "\n",
    "# # Save processed data\n",
    "# # final_data.write.parquet(\"processed_profile_data.parquet\")\n",
    "# display(final_data.limit(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "447194fa-fb3a-46db-b1fa-748b46ade76a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(processed_data.limit(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d7fd1e63-b376-4410-bf7b-5e977b1b68d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def to_dense_vector(embeddings_array):\n",
    "    return Vectors.dense(embeddings_array)\n",
    "\n",
    "# Register a UDF to convert arrays to dense vectors\n",
    "to_dense_udf = udf(lambda x: to_dense_vector(x), VectorUDT())\n",
    "\n",
    "# Apply the UDF to the embeddings column (adjust column name as needed)\n",
    "processed_data = processed_data.withColumn(\n",
    "    \"about_embeddings_dense\", \n",
    "    to_dense_udf(f.expr(\"about_embeddings.embeddings[0]\"))\n",
    ")\n",
    "\n",
    "\n",
    "# Assemble features\n",
    "assembler = VectorAssembler(inputCols=[\n",
    "    \"about_embeddings_dense\", \"num_education\", \"num_experience\", \"num_languages\",\n",
    "    \"total_followers\", \"recommendations\",\n",
    "], outputCol=\"features\", handleInvalid=\"skip\")\n",
    "\n",
    "final_data = assembler.transform(processed_data)\n",
    "\n",
    "# Select relevant columns\n",
    "final_data = final_data.select(\"features\", \"filled_precent\")\n",
    "\n",
    "# Save processed data\n",
    "# final_data.write.parquet(\"processed_profile_data.parquet\")\n",
    "display(final_data.limit(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b3107492-59a9-4d94-9d62-3b8ad9b418d6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# final_data.write.parquet(\"processed_profile_data.parquet\")\n",
    "final_data.write.parquet(\"/Workspace/Users/harel.oved@campus.technion.ac.il/processed_data.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d5d34993-c49b-4649-b12b-e7533431e28b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "final_data = spark.read.parquet('/Workspace/Users/harel.oved@campus.technion.ac.il/processed_data.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a2cca20a-44a1-412c-91c5-982f87c23c37",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sample = profiles_df.select('profile_score').sample(False, 0.1).toPandas()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(sample['profile_score'], bins=30, edgecolor='k', alpha=0.7)\n",
    "plt.title('Histogram of Profile Scores')\n",
    "plt.xlabel('Profile Score')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f9cfe18-ebae-4578-9e90-45d68171d5c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Scraped Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73d6a7f4-8d8a-4c86-8e82-5833f04a2305",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Job Titles and Locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "88d9f76c-c578-4b11-bc93-c04a000540c6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "jobs = profiles.select('name', 'id', 'city', 'country_code', f.col('current_company').getField('name').alias('company_name'), f.col('experience')[0].getField('title').alias('job_title'), 'position')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ecaad7f1-0949-4dba-bc42-a33b8f5bbb86",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "jobs.display(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b1dde82-9c1c-4bb7-8869-46bd9e976297",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####Clustering job titles into meta job titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f03fd01-bb46-472d-b31e-781966de86c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, lower, when, split\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.ml.feature import Word2Vec, Tokenizer\n",
    "\n",
    "# Create a DataFrame with the specified centroids\n",
    "centroids_data = [\n",
    "    ('Leadership',), ('Product',), ('Engineering',), ('DataScience',), ('Operations',),\n",
    "    ('Marketing',), ('Sales',), ('Design',), ('Support',), ('Finance',),\n",
    "    ('Resources',), ('Research',), ('Healthcare',), ('Education',), ('Security',),\n",
    "    ('Logistics',), ('Legal',), ('Quality',), ('Management',), ('Content',)\n",
    "]\n",
    "\n",
    "centroids_df = spark.createDataFrame(centroids_data, ['processed_title'])\n",
    "\n",
    "# Preprocess job titles\n",
    "job_titles_df = jobs.select(\n",
    "    when(col('job_title').isNotNull(), lower(col('job_title')))\n",
    "    .otherwise(lower(col('position')))\n",
    "    .alias('processed_title')\n",
    ")\n",
    "job_titles_df = job_titles_df.dropna()\n",
    "tokenizer = Tokenizer(inputCol=\"processed_title\", outputCol=\"tokened_title\")\n",
    "w2v = Word2Vec(inputCol=\"tokened_title\", outputCol=\"vector\", vectorSize=200, minCount=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Build the pipeline\n",
    "pipeline = Pipeline(stages=[tokenizer, w2v])\n",
    "\n",
    "# Train the pipeline model\n",
    "model_vectorize = pipeline.fit(job_titles_df)\n",
    "\n",
    "\n",
    "\n",
    "# Create embeddings for job titles and centroids\n",
    "jobs_with_vectors = model_vectorize.transform(job_titles_df)\n",
    "centroids_with_vectors = model_vectorize.transform(centroids_df)\n",
    "\n",
    "\n",
    "jobs_temp = jobs_with_vectors.withColumnRenamed('vector', 'job_vector')\n",
    "jobs_temp = jobs_temp.withColumnRenamed('processed_title', 'job_title')\n",
    "\n",
    "centroids_temp = centroids_with_vectors.withColumnRenamed('processed_title', 'meta_job')\n",
    "centroids_temp = centroids_temp.withColumnRenamed('vector', 'centroid_vector')\n",
    "\n",
    "joined = jobs_temp.join(centroids_temp)\n",
    "display(joined.limit(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "09660b36-5c96-4000-8114-13a3e16c1734",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.linalg import DenseVector\n",
    "import math\n",
    "\n",
    "# Define a function to calculate cosine similarity\n",
    "def cosine_similarity(v1, v2):\n",
    "    if v1 is None or v2 is None:\n",
    "        return None\n",
    "    dot_product = float(v1.dot(v2))  # Dot product of the two vectors\n",
    "    norm_v1 = math.sqrt(v1.dot(v1))  # Magnitude (norm) of v1\n",
    "    norm_v2 = math.sqrt(v2.dot(v2))  # Magnitude (norm) of v2\n",
    "    if norm_v1 == 0 or norm_v2 == 0:\n",
    "        return None  # Avoid division by zero\n",
    "    return dot_product / (norm_v1 * norm_v2)\n",
    "\n",
    "# Register the function as a UDF\n",
    "cosine_similarity_udf = udf(cosine_similarity, StringType())\n",
    "\n",
    "# Add a new column to compute cosine similarity\n",
    "joined = joined.withColumn(\n",
    "    \"cosine_similarity\",\n",
    "    cosine_similarity_udf(col(\"job_vector\"), col(\"centroid_vector\"))\n",
    ")\n",
    "\n",
    "# Show the result\n",
    "joined.display()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "074e77d5-0325-4fab-8133-aaee42b0a084",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "window_spec = Window.partitionBy(\"job_title\").orderBy(col(\"cosine_similarity\").desc())\n",
    "\n",
    "# Rank centroids for each job and select the closest one\n",
    "ranked_df = joined.withColumn(\"rank\", f.row_number().over(window_spec))\n",
    "\n",
    "# Filter for the closest centroid\n",
    "closest_centroids = ranked_df.filter(col(\"rank\") == 1)\n",
    "\n",
    "# Select relevant columns\n",
    "result_df = closest_centroids.select(\n",
    "    col(\"job_title\"),\n",
    "    col(\"meta_job\").alias(\"closest_centroid\"),\n",
    "    col(\"cosine_similarity\")\n",
    ")\n",
    "\n",
    "# Display the result\n",
    "result_df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4419c0cc-1872-4b67-987d-4b524c095f44",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "profiles_with_state = profiles.withColumn(\n",
    "    \"state\",\n",
    "    split(col(\"city\"), \", \")[1]  # The second element is the state\n",
    ")\n",
    "\n",
    "# Show the results\n",
    "states_df = profiles_with_state.select(\"state\").dropDuplicates().dropna()\n",
    "states_df.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "36ec5e25-f521-4c0d-b2b0-3985d1ef22c6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####Scraping "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "28ff6587-7102-4f22-a13a-a8c601153c5c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install selenium\n",
    "%pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9ccd16ea-b9c8-4d79-9df3-2f464f6a8e99",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## this notebook is way to big so maybe move the scraping process to a new notebook and save the data in a parquet file to read from this notebook?\n",
    "Yes we should divide the parts to differnet files it will be good to display them separately in the git\n",
    "\n",
    "exactly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc4c94e6-1391-4176-bf49-22bcc41d3a8d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import random\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def get_html(job, state):\n",
    "    print(state, job)\n",
    "    # Define the job title and location\n",
    "    paginaton_url = 'https://www.indeed.com/jobs?q={}&l={}&'\n",
    "    driver.get(paginaton_url.format(job, state))\n",
    "    time.sleep(random.randint(2, 6))\n",
    "    return driver.page_source\n",
    "\n",
    "def get_count(html):\n",
    "    # Parse the HTML using BeautifulSoup\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    # Find the job results container\n",
    "    job_count_div = soup.find(\"div\", class_=\"jobsearch-JobCountAndSortPane-jobCount\")\n",
    "\n",
    "    count = job_count_div.find(\"span\").text.split(' ')[0][:-1]\n",
    "    return count\n",
    "\n",
    "centroids_data = [\n",
    "    ('Leadership',), ('Product',), ('Engineering',), ('DataScience',), ('Operations',),\n",
    "    ('Marketing',), ('Sales',), ('Design',), ('Support',), ('Finance',),\n",
    "    ('Resources',), ('Research',), ('Healthcare',), ('Education',), ('Security',),\n",
    "    ('Logistics',), ('Legal',), ('Quality',), ('Management',), ('Content',)\n",
    "]\n",
    "\n",
    "centroids_df = spark.createDataFrame(centroids_data, ['processed_title'])\n",
    "job_state_df = states_df.join(centroids_df)\n",
    "\n",
    "df = job_state_df.toPandas()\n",
    "\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument(\"--user-data-dir=/tmp/chrome_user_data\")\n",
    "\n",
    "driver = webdriver.Chrome(options=options)\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    state = row['state']\n",
    "    job = row['processed_title']\n",
    "    html = get_html(job, state)\n",
    "    count = get_count(html)\n",
    "    print(count)\n",
    "\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8afcc7df-668f-421a-8237-17a41fc72644",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Good Profiles Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5e9975a2-2a9f-463d-bff2-61623935763d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### i want to predit a numeric score and not binary label -- will be better for the final stage of suggesting improvemnts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "71148ed7-d8e4-4a42-a657-626a6c78d63f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Training the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "64dc7421-9343-4d12-a997-5913ff8f36a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "possible models:\n",
    "- Decision Tree Regressor\n",
    "- Random Forest Regressor\n",
    "- Gradient-Boosted Trees Regressor\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "99be653a-17ec-4776-a9d1-2e0161bdc57b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3100db3c-4d73-4fab-bb62-e8f899d6d172",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e622f6df-80aa-47d0-a5fa-7518ffd81013",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c20c6171-2c93-4759-a167-ac92d564d0ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "when checking accuracy - accepted score should be between (real_score-5, real_score+5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef28b933-4f71-4731-a32f-6bf219267eaf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Profile Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ab612f1-eee5-42ea-baed-78284e6c9683",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 'about' Section Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a83e58b4-215f-40fe-a79a-0989590d452e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# take: about (if not null), position, job title, reccomendations \n",
    "# --> return: a sentence or two describing the person and job (in a new column called 'new_about')\n",
    "# if all null: return message 'could not generate a short bio -- add more information to your profile' (put null in 'new_about' and add message in a new column called 'about_message')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b0db48a2-457c-45be-9c59-118647dca859",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Improvements and Suggetions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9bf77270-eb22-40f3-b9ea-1af23ad698a7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "score ranking:\n",
    "- excellent score - 90+ and no suggestions\n",
    "- high score - 90+ and atleast one suggestion\n",
    "- medium high score - 60-90\n",
    "- medium score - 40-60\n",
    "- medium low score - 20-40\n",
    "- low score - 20>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "187a0312-7fa5-4b34-9bd9-af75a6f1d18c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "score_messages = {\n",
    "    'excellent_score': 'Your profile is excellent, keep it up!',\n",
    "    'high_score': 'Your profile is very strong, Check the suggestions to make it excellent',\n",
    "    'medium_high_score': 'Your profile is good, Try to follow the suggestions to make it even better',\n",
    "    'medium_score': 'Your profile could use a few improvements, Try to follow the suggestions to make it even better',\n",
    "    'medium_low_score': 'Your profile needs to improve, Try to follow the suggestion to make it better',\n",
    "    'low_score': 'Your profile is weak, Try to follow the suggestion to make it better',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "15d4234a-8ae9-4b09-8fc8-a9eafb9a5681",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "missing_field_messages = {\n",
    "    'no_experience': 'Add previous/current comapnies you worked in', \n",
    "    'no_education': 'List your degrees and schools you graduated from',\n",
    "    'no_about': 'Add a short bio about yourself, here is a suggestion: ',\n",
    "    'suggested_about': 'Try out this about section: ',\n",
    "    'no_company': 'Add the compant you currently work in',\n",
    "    'no_languages': 'List all the languages you know and the level of knowledge',\n",
    "    'no_position': 'Add the position you are currently in',\n",
    "    'no_posts': 'Try to be more active with you account',\n",
    "    'no_recommendations': 'Ask a colleague to write a few words about you',\n",
    "    'missing_experience': 'There is a gap in your resume, Don\\'t forget to add all of the previous comapnies you worked in',\n",
    "    'low_followers': 'Ask your colleagues and friends to follow you on LinkedIn!'\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5321e47c-06aa-45f3-a950-fb0521d824e8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# placeholder name for the predictions: predicted_df (has all the previous columns + score predictions)\n",
    "\n",
    "predicted_df = predicted_df.withColumn(\n",
    "  'score_rank', \n",
    "  f.when(f.col('score') < 20, 'low_score'\n",
    "  ).when(f.col('score') < 40, 'medium_low_score'\n",
    "  ).when(f.col('score') < 60, 'medium_score'\n",
    "  ).when(f.col('score') < 90, 'medium_high_score'\n",
    "  ).when(f.col('filled_percent') < 100, 'high_score'\n",
    "  ).otherwise('excellent_score')\n",
    ")\n",
    "\n",
    "predicted_df = predicted_df.withColumn(\n",
    "  'score_message',\n",
    "  score_messages.get(f.col('score_rank'))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "82696174-9da2-4377-9508-61ae90a1ce13",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# find if there are gaps in the experience array (name new column: 'gap_in_experience')\n",
    "# TODO: Binary or explicit time period? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "70f31f75-c24a-4033-83d0-67280a4a84f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "predicted_df = predicted_df.withColumn('suggestions', f.array())\n",
    "\n",
    "predicted_df = predicted_df.withColumn(\n",
    "  'suggestions',\n",
    "  f.array(\n",
    "    f.when(\n",
    "      f.size(f.col('education')) == 0, \n",
    "      missing_field_messages.get('no_education')),\n",
    "    f.when(\n",
    "      f.size(f.col('current_company')) == 0, \n",
    "      missing_field_messages.get('no_company')),\n",
    "    f.when(\n",
    "      f.size(f.col('languages')) == 0, \n",
    "      missing_field_messages.get('no_languages')),\n",
    "    f.when(\n",
    "      f.size(f.col('posts')) == 0, \n",
    "      missing_field_messages.get('no_posts')),\n",
    "    f.when(\n",
    "      f.col('recommendations_count') == 0, \n",
    "      missing_field_messages.get('no_recommendations')),\n",
    "    f.when(\n",
    "      f.col('about').isNull() & f.col('new_about').isNotNull(), \n",
    "      missing_field_messages.get('no_about') + f.col('new_about')),\n",
    "    f.when(\n",
    "      f.col('about').isNotNull() & f.col('new_about').isNotNull() & f.col('score') < 90, \n",
    "      missing_field_messages.get('suggested_about') + f.col('new_about')),\n",
    "    f.when(\n",
    "      f.col('about_message').isNotNull(), \n",
    "      f.col('about_message')),\n",
    "    f.when(\n",
    "      f.col('position').isNull(),\n",
    "      missing_field_messages.get('no_position')),\n",
    "    f.when(\n",
    "      f.col('followers') < 20,\n",
    "      missing_field_messages.get('low_followers')),\n",
    "    f.when(\n",
    "      f.size(f.col('experience')) == 0, \n",
    "      missing_field_messages.get('no_experience')), \n",
    "    f.when(\n",
    "      f.col('gap_in_experience').isNotNull(), # TODO: adapt to binary or time period\n",
    "      missing_field_messages.get('missing_experience'))\n",
    "  )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "31fb62a8-cb9f-4367-921c-2827fa84c4fe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "optemized_df = predicted_df.select('name', 'id', 'url', 'score_rank', 'score_message', 'suggestions')\n",
    "display(optemized_df)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "project",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
